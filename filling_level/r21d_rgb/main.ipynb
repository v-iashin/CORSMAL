{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reproduce our experiments just run the first cell. Only GPU back-end is supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "init_time: 200903214601\n    task: flvl\n    output_dim: 3\n    model_type: GRU\n    bi_dir: False\n    device: cuda:0\n    data_root: ./r21d_rgb_features\n    drop_p: 0.0\n    batch_size: 64\n    input_dim: 512\n    hidden_dim: 512\n    n_layers: 3\n    num_epochs: 30\n    seed: 1337\n{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'test': [10, 11, 12]}\n(train @ 1): L: 1.178569; A: 0.546053; R: 0.546053; P: 0.529675; F1: 0.530272\n(valid @ 1): L: 1.173517; A: 0.473684; R: 0.473684; P: 0.429518; F1: 0.412183\n(train @ 2): L: 0.945246; A: 0.482456; R: 0.482456; P: 0.561379; F1: 0.449099\n(valid @ 2): L: 0.998492; A: 0.451754; R: 0.451754; P: 0.560523; F1: 0.382529\n(train @ 3): L: 0.912040; A: 0.493421; R: 0.493421; P: 0.569558; F1: 0.464476\n(valid @ 3): L: 1.250378; A: 0.491228; R: 0.491228; P: 0.525911; F1: 0.476556\n(train @ 4): L: 0.975016; A: 0.565789; R: 0.565789; P: 0.593927; F1: 0.559378\n(valid @ 4): L: 0.975711; A: 0.526316; R: 0.526316; P: 0.608895; F1: 0.494469\n(train @ 5): L: 0.816230; A: 0.616228; R: 0.616228; P: 0.627962; F1: 0.610786\n(valid @ 5): L: 0.883267; A: 0.627193; R: 0.627193; P: 0.655977; F1: 0.630118\n(train @ 6): L: 0.770158; A: 0.679825; R: 0.679825; P: 0.714616; F1: 0.674596\n(valid @ 6): L: 1.025684; A: 0.482456; R: 0.482456; P: 0.594568; F1: 0.424975\n(train @ 7): L: 0.869398; A: 0.524123; R: 0.524123; P: 0.540238; F1: 0.527220\n(valid @ 7): L: 1.206449; A: 0.561404; R: 0.561404; P: 0.598086; F1: 0.533835\n(train @ 8): L: 0.915536; A: 0.576754; R: 0.576754; P: 0.577679; F1: 0.576584\n(valid @ 8): L: 0.966880; A: 0.469298; R: 0.469298; P: 0.765220; F1: 0.335613\n(train @ 9): L: 0.824403; A: 0.543860; R: 0.543860; P: 0.550845; F1: 0.545533\n(valid @ 9): L: 0.848068; A: 0.649123; R: 0.649123; P: 0.651634; F1: 0.643673\n(train @ 10): L: 0.738878; A: 0.664474; R: 0.664474; P: 0.665795; F1: 0.663683\n(valid @ 10): L: 0.878918; A: 0.552632; R: 0.552632; P: 0.566158; F1: 0.485874\n(train @ 11): L: 0.738103; A: 0.572368; R: 0.572368; P: 0.577047; F1: 0.573897\n(valid @ 11): L: 0.733031; A: 0.666667; R: 0.666667; P: 0.665864; F1: 0.665366\n(train @ 12): L: 0.625226; A: 0.728070; R: 0.728070; P: 0.746197; F1: 0.728354\n(valid @ 12): L: 0.761645; A: 0.627193; R: 0.627193; P: 0.636683; F1: 0.619439\n(train @ 13): L: 0.633128; A: 0.686404; R: 0.686404; P: 0.690656; F1: 0.686956\n(valid @ 13): L: 0.953445; A: 0.614035; R: 0.614035; P: 0.644528; F1: 0.593868\n(train @ 14): L: 0.732491; A: 0.657895; R: 0.657895; P: 0.659091; F1: 0.658130\n(valid @ 14): L: 0.887071; A: 0.600877; R: 0.600877; P: 0.604106; F1: 0.582563\n(train @ 15): L: 0.750678; A: 0.589912; R: 0.589912; P: 0.589181; F1: 0.586655\n(valid @ 15): L: 1.024972; A: 0.561404; R: 0.561404; P: 0.596023; F1: 0.535469\n(train @ 16): L: 0.846797; A: 0.574561; R: 0.574561; P: 0.571551; F1: 0.571096\n(valid @ 16): L: 0.705839; A: 0.675439; R: 0.675439; P: 0.701063; F1: 0.668037\n(train @ 17): L: 0.661203; A: 0.684211; R: 0.684211; P: 0.698582; F1: 0.682518\n(valid @ 17): L: 0.716377; A: 0.640351; R: 0.640351; P: 0.706215; F1: 0.607625\n(train @ 18): L: 0.682743; A: 0.690789; R: 0.690789; P: 0.731616; F1: 0.690801\n(valid @ 18): L: 0.793429; A: 0.666667; R: 0.666667; P: 0.735774; F1: 0.646966\n(train @ 19): L: 0.625881; A: 0.752193; R: 0.752193; P: 0.770286; F1: 0.754720\n(valid @ 19): L: 0.668152; A: 0.692982; R: 0.692982; P: 0.703796; F1: 0.693556\n(train @ 20): L: 0.555781; A: 0.730263; R: 0.730263; P: 0.733387; F1: 0.729698\n(valid @ 20): L: 0.705547; A: 0.666667; R: 0.666667; P: 0.674496; F1: 0.660428\n(train @ 21): L: 0.495258; A: 0.791667; R: 0.791667; P: 0.801994; F1: 0.792160\n(valid @ 21): L: 0.692874; A: 0.701754; R: 0.701754; P: 0.710954; F1: 0.700356\n(train @ 22): L: 0.517639; A: 0.780702; R: 0.780702; P: 0.784895; F1: 0.780931\n(valid @ 22): L: 0.697780; A: 0.649123; R: 0.649123; P: 0.669769; F1: 0.637517\n(train @ 23): L: 0.516263; A: 0.750000; R: 0.750000; P: 0.749733; F1: 0.749794\n(valid @ 23): L: 0.727997; A: 0.622807; R: 0.622807; P: 0.641164; F1: 0.607455\n(train @ 24): L: 0.450271; A: 0.815789; R: 0.815789; P: 0.819883; F1: 0.815256\n(valid @ 24): L: 0.716183; A: 0.697368; R: 0.697368; P: 0.730923; F1: 0.692023\n(train @ 25): L: 0.411158; A: 0.833333; R: 0.833333; P: 0.836422; F1: 0.833508\n(valid @ 25): L: 0.813549; A: 0.653509; R: 0.653509; P: 0.672103; F1: 0.638276\n(train @ 26): L: 0.491303; A: 0.771930; R: 0.771930; P: 0.775466; F1: 0.771328\n(valid @ 26): L: 0.639806; A: 0.697368; R: 0.697368; P: 0.701750; F1: 0.696628\n(train @ 27): L: 0.410792; A: 0.842105; R: 0.842105; P: 0.843906; F1: 0.842227\n(valid @ 27): L: 0.722009; A: 0.675439; R: 0.675439; P: 0.674637; F1: 0.672107\n(train @ 28): L: 0.364842; A: 0.837719; R: 0.837719; P: 0.847286; F1: 0.838229\n(valid @ 28): L: 0.956086; A: 0.605263; R: 0.605263; P: 0.683153; F1: 0.569463\n(train @ 29): L: 0.377716; A: 0.833333; R: 0.833333; P: 0.838259; F1: 0.833997\n(valid @ 29): L: 0.726179; A: 0.714912; R: 0.714912; P: 0.739452; F1: 0.717570\n(train @ 30): L: 0.324321; A: 0.861842; R: 0.861842; P: 0.864596; F1: 0.861941\n(valid @ 30): L: 0.788355; A: 0.671053; R: 0.671053; P: 0.706219; F1: 0.660922\nBest val Metric 0.717570 @ 29\n\n{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'test': [10, 11, 12]}\n(train @ 1): L: 1.301895; A: 0.438596; R: 0.438596; P: 0.429551; F1: 0.433183\n(valid @ 1): L: 1.013824; A: 0.504386; R: 0.504386; P: 0.460449; F1: 0.447305\n(train @ 2): L: 1.027758; A: 0.447368; R: 0.447368; P: 0.376886; F1: 0.408407\n(valid @ 2): L: 0.908666; A: 0.491228; R: 0.491228; P: 0.603283; F1: 0.435783\n(train @ 3): L: 0.936614; A: 0.458333; R: 0.458333; P: 0.513744; F1: 0.432105\n(valid @ 3): L: 0.962074; A: 0.609649; R: 0.609649; P: 0.663570; F1: 0.604208\n(train @ 4): L: 0.988010; A: 0.524123; R: 0.524123; P: 0.553600; F1: 0.513818\n(valid @ 4): L: 1.139749; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n(train @ 5): L: 0.869700; A: 0.537281; R: 0.537281; P: 0.540217; F1: 0.531868\n(valid @ 5): L: 0.874917; A: 0.570175; R: 0.570175; P: 0.787303; F1: 0.492115\n(train @ 6): L: 0.829537; A: 0.578947; R: 0.578947; P: 0.585625; F1: 0.574587\n(valid @ 6): L: 0.846998; A: 0.574561; R: 0.574561; P: 0.641098; F1: 0.560248\n(train @ 7): L: 0.885264; A: 0.524123; R: 0.524123; P: 0.528578; F1: 0.525423\n(valid @ 7): L: 0.862722; A: 0.557018; R: 0.557018; P: 0.611308; F1: 0.537246\n(train @ 8): L: 0.873105; A: 0.561404; R: 0.561404; P: 0.565594; F1: 0.555213\n(valid @ 8): L: 1.194860; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n(train @ 9): L: 0.943991; A: 0.484649; R: 0.484649; P: 0.488265; F1: 0.484220\n(valid @ 9): L: 0.787683; A: 0.565789; R: 0.565789; P: 0.703935; F1: 0.475099\n(train @ 10): L: 0.793125; A: 0.557018; R: 0.557018; P: 0.557744; F1: 0.554714\n(valid @ 10): L: 0.804448; A: 0.535088; R: 0.535088; P: 0.689630; F1: 0.455665\n(train @ 11): L: 0.765455; A: 0.563596; R: 0.563596; P: 0.561069; F1: 0.560767\n(valid @ 11): L: 0.755720; A: 0.605263; R: 0.605263; P: 0.666667; F1: 0.568798\n(train @ 12): L: 0.684962; A: 0.666667; R: 0.666667; P: 0.674251; F1: 0.666748\n(valid @ 12): L: 0.774505; A: 0.583333; R: 0.583333; P: 0.743124; F1: 0.498645\n(train @ 13): L: 0.698410; A: 0.666667; R: 0.666667; P: 0.671241; F1: 0.667352\n(valid @ 13): L: 0.789138; A: 0.596491; R: 0.596491; P: 0.762468; F1: 0.509488\n(train @ 14): L: 0.770372; A: 0.594298; R: 0.594298; P: 0.596052; F1: 0.592366\n(valid @ 14): L: 0.737545; A: 0.684211; R: 0.684211; P: 0.715392; F1: 0.684741\n(train @ 15): L: 0.782423; A: 0.589912; R: 0.589912; P: 0.588971; F1: 0.587272\n(valid @ 15): L: 0.774802; A: 0.605263; R: 0.605263; P: 0.697842; F1: 0.584005\n(train @ 16): L: 0.788267; A: 0.539474; R: 0.539474; P: 0.539955; F1: 0.538504\n(valid @ 16): L: 0.705279; A: 0.688596; R: 0.688596; P: 0.778204; F1: 0.676539\n(train @ 17): L: 0.685280; A: 0.675439; R: 0.675439; P: 0.691834; F1: 0.673904\n(valid @ 17): L: 0.626811; A: 0.745614; R: 0.745614; P: 0.794924; F1: 0.744771\n(train @ 18): L: 0.671684; A: 0.690789; R: 0.690789; P: 0.718024; F1: 0.692634\n(valid @ 18): L: 0.591396; A: 0.754386; R: 0.754386; P: 0.761030; F1: 0.753826\n(train @ 19): L: 0.677188; A: 0.699561; R: 0.699561; P: 0.708241; F1: 0.700730\n(valid @ 19): L: 0.626528; A: 0.714912; R: 0.714912; P: 0.776813; F1: 0.712229\n(train @ 20): L: 0.592648; A: 0.732456; R: 0.732456; P: 0.742494; F1: 0.731789\n(valid @ 20): L: 0.622718; A: 0.719298; R: 0.719298; P: 0.739914; F1: 0.721970\n(train @ 21): L: 0.577414; A: 0.730263; R: 0.730263; P: 0.738305; F1: 0.729502\n(valid @ 21): L: 0.542145; A: 0.771930; R: 0.771930; P: 0.788155; F1: 0.773922\n(train @ 22): L: 0.563592; A: 0.765351; R: 0.765351; P: 0.767719; F1: 0.764969\n(valid @ 22): L: 0.661534; A: 0.671053; R: 0.671053; P: 0.743377; F1: 0.657216\n(train @ 23): L: 0.615297; A: 0.690789; R: 0.690789; P: 0.689311; F1: 0.689747\n(valid @ 23): L: 0.795266; A: 0.649123; R: 0.649123; P: 0.739639; F1: 0.631892\n(train @ 24): L: 0.535322; A: 0.752193; R: 0.752193; P: 0.753068; F1: 0.751889\n(valid @ 24): L: 0.540217; A: 0.767544; R: 0.767544; P: 0.790747; F1: 0.768490\n(train @ 25): L: 0.482843; A: 0.809211; R: 0.809211; P: 0.811965; F1: 0.809805\n(valid @ 25): L: 0.695504; A: 0.653509; R: 0.653509; P: 0.731016; F1: 0.627161\n(train @ 26): L: 0.536420; A: 0.760965; R: 0.760965; P: 0.764899; F1: 0.761031\n(valid @ 26): L: 0.518378; A: 0.763158; R: 0.763158; P: 0.795898; F1: 0.763127\n(train @ 27): L: 0.544617; A: 0.767544; R: 0.767544; P: 0.780077; F1: 0.769872\n(valid @ 27): L: 0.651974; A: 0.736842; R: 0.736842; P: 0.795489; F1: 0.733790\n(train @ 28): L: 0.465277; A: 0.778509; R: 0.778509; P: 0.785290; F1: 0.778205\n(valid @ 28): L: 0.554163; A: 0.714912; R: 0.714912; P: 0.729318; F1: 0.717677\n(train @ 29): L: 0.464071; A: 0.793860; R: 0.793860; P: 0.798153; F1: 0.794643\n(valid @ 29): L: 0.517687; A: 0.745614; R: 0.745614; P: 0.785370; F1: 0.740142\n(train @ 30): L: 0.407575; A: 0.811404; R: 0.811404; P: 0.813043; F1: 0.811440\n(valid @ 30): L: 0.512614; A: 0.750000; R: 0.750000; P: 0.786883; F1: 0.750075\nBest val Metric 0.773922 @ 21\n\n{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'test': [10, 11, 12]}\n(train @ 1): L: 1.377982; A: 0.440789; R: 0.440789; P: 0.437169; F1: 0.438407\n(valid @ 1): L: 1.057246; A: 0.592105; R: 0.592105; P: 0.655556; F1: 0.553748\n(train @ 2): L: 1.021019; A: 0.449561; R: 0.449561; P: 0.450246; F1: 0.425644\n(valid @ 2): L: 0.975680; A: 0.438596; R: 0.438596; P: 0.338075; F1: 0.290083\n(train @ 3): L: 0.939833; A: 0.497807; R: 0.497807; P: 0.572891; F1: 0.465497\n(valid @ 3): L: 0.951345; A: 0.600877; R: 0.600877; P: 0.650751; F1: 0.593477\n(train @ 4): L: 0.933772; A: 0.539474; R: 0.539474; P: 0.604842; F1: 0.507084\n(valid @ 4): L: 1.217686; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n(train @ 5): L: 0.947217; A: 0.497807; R: 0.497807; P: 0.508636; F1: 0.484260\n(valid @ 5): L: 0.904597; A: 0.622807; R: 0.622807; P: 0.643620; F1: 0.603689\n(train @ 6): L: 0.894864; A: 0.517544; R: 0.517544; P: 0.516568; F1: 0.511595\n(valid @ 6): L: 0.947655; A: 0.535088; R: 0.535088; P: 0.629321; F1: 0.503307\n(train @ 7): L: 0.923316; A: 0.528509; R: 0.528509; P: 0.529581; F1: 0.528468\n(valid @ 7): L: 0.940572; A: 0.486842; R: 0.486842; P: 0.580893; F1: 0.443707\n(train @ 8): L: 0.902744; A: 0.526316; R: 0.526316; P: 0.533870; F1: 0.517490\n(valid @ 8): L: 1.195367; A: 0.416667; R: 0.416667; P: 0.176211; F1: 0.247678\n(train @ 9): L: 0.941397; A: 0.510965; R: 0.510965; P: 0.520207; F1: 0.506516\n(valid @ 9): L: 0.854287; A: 0.657895; R: 0.657895; P: 0.780651; F1: 0.603048\n(train @ 10): L: 0.847215; A: 0.557018; R: 0.557018; P: 0.577209; F1: 0.544681\n(valid @ 10): L: 0.902432; A: 0.482456; R: 0.482456; P: 0.587203; F1: 0.428425\n(train @ 11): L: 0.769256; A: 0.572368; R: 0.572368; P: 0.579661; F1: 0.569156\n(valid @ 11): L: 0.930880; A: 0.508772; R: 0.508772; P: 0.678957; F1: 0.414486\n(train @ 12): L: 0.746593; A: 0.583333; R: 0.583333; P: 0.594255; F1: 0.583595\n(valid @ 12): L: 0.897945; A: 0.526316; R: 0.526316; P: 0.356037; F1: 0.395789\n(train @ 13): L: 0.764677; A: 0.620614; R: 0.620614; P: 0.629416; F1: 0.621291\n(valid @ 13): L: 0.859915; A: 0.596491; R: 0.596491; P: 0.793953; F1: 0.540129\n(train @ 14): L: 0.741103; A: 0.609649; R: 0.609649; P: 0.617097; F1: 0.605950\n(valid @ 14): L: 0.900401; A: 0.609649; R: 0.609649; P: 0.666535; F1: 0.600663\n(train @ 15): L: 0.725685; A: 0.587719; R: 0.587719; P: 0.591148; F1: 0.584549\n(valid @ 15): L: 0.736241; A: 0.605263; R: 0.605263; P: 0.651248; F1: 0.594755\n(train @ 16): L: 0.753138; A: 0.550439; R: 0.550439; P: 0.550807; F1: 0.549889\n(valid @ 16): L: 0.792264; A: 0.657895; R: 0.657895; P: 0.739943; F1: 0.642300\n(train @ 17): L: 0.669185; A: 0.664474; R: 0.664474; P: 0.675717; F1: 0.660887\n(valid @ 17): L: 0.710851; A: 0.697368; R: 0.697368; P: 0.736801; F1: 0.696629\n(train @ 18): L: 0.671767; A: 0.677632; R: 0.677632; P: 0.696606; F1: 0.675052\n(valid @ 18): L: 0.639589; A: 0.750000; R: 0.750000; P: 0.764757; F1: 0.750570\n(train @ 19): L: 0.639242; A: 0.719298; R: 0.719298; P: 0.727438; F1: 0.719395\n(valid @ 19): L: 0.669512; A: 0.706140; R: 0.706140; P: 0.776008; F1: 0.700645\n(train @ 20): L: 0.582668; A: 0.752193; R: 0.752193; P: 0.755103; F1: 0.751590\n(valid @ 20): L: 0.637833; A: 0.741228; R: 0.741228; P: 0.759746; F1: 0.741974\n(train @ 21): L: 0.590702; A: 0.714912; R: 0.714912; P: 0.720973; F1: 0.714567\n(valid @ 21): L: 0.616579; A: 0.706140; R: 0.706140; P: 0.775056; F1: 0.700585\n(train @ 22): L: 0.625819; A: 0.666667; R: 0.666667; P: 0.667060; F1: 0.665194\n(valid @ 22): L: 0.820628; A: 0.570175; R: 0.570175; P: 0.683047; F1: 0.525314\n(train @ 23): L: 0.668496; A: 0.655702; R: 0.655702; P: 0.659711; F1: 0.655731\n(valid @ 23): L: 0.891406; A: 0.561404; R: 0.561404; P: 0.680759; F1: 0.515552\n(train @ 24): L: 0.598336; A: 0.695175; R: 0.695175; P: 0.696584; F1: 0.695067\n(valid @ 24): L: 0.574932; A: 0.736842; R: 0.736842; P: 0.756771; F1: 0.738639\n(train @ 25): L: 0.535388; A: 0.763158; R: 0.763158; P: 0.772541; F1: 0.763513\n(valid @ 25): L: 0.649807; A: 0.666667; R: 0.666667; P: 0.736774; F1: 0.648164\n(train @ 26): L: 0.665631; A: 0.703947; R: 0.703947; P: 0.706584; F1: 0.702317\n(valid @ 26): L: 0.668865; A: 0.750000; R: 0.750000; P: 0.775557; F1: 0.746760\n(train @ 27): L: 0.579283; A: 0.734649; R: 0.734649; P: 0.752564; F1: 0.734203\n(valid @ 27): L: 0.761145; A: 0.692982; R: 0.692982; P: 0.759742; F1: 0.683282\n(train @ 28): L: 0.551579; A: 0.732456; R: 0.732456; P: 0.740355; F1: 0.732680\n(valid @ 28): L: 0.554384; A: 0.728070; R: 0.728070; P: 0.761678; F1: 0.727389\n(train @ 29): L: 0.486401; A: 0.785088; R: 0.785088; P: 0.793712; F1: 0.785075\n(valid @ 29): L: 0.589414; A: 0.697368; R: 0.697368; P: 0.738408; F1: 0.694528\n(train @ 30): L: 0.478738; A: 0.767544; R: 0.767544; P: 0.773324; F1: 0.766615\n(valid @ 30): L: 0.719634; A: 0.697368; R: 0.697368; P: 0.726151; F1: 0.693803\nBest val Metric 0.750570 @ 18\n\nAverage of Best Metrics on Each Valid Set: 0.747354, 200903214601_200905220428\n"
    }
   ],
   "source": [
    "# Reproduce the best experiment\n",
    "import argparse\n",
    "from main import Config, run_kfold\n",
    "from time import localtime, strftime\n",
    "\n",
    "exp_name = 200903214601\n",
    "cfg = Config()\n",
    "cfg.load_from(path=f'./predictions/{exp_name}/cfg.txt')\n",
    "# replacing the time with the old_time + current_time such that there is no collision\n",
    "cfg.init_time = f'{cfg.init_time}_{strftime(\"%y%m%d%H%M%S\", localtime())}'\n",
    "run_kfold(cfg) # Expected average of Best Metrics on Each Valid Set: 0.747354 @ 200903214601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'test': [10, 11, 12]}\n(train @ 1): L: 1.178569; A: 0.546053; R: 0.546053; P: 0.529675; F1: 0.530272\n(valid @ 1): L: 1.173517; A: 0.473684; R: 0.473684; P: 0.429518; F1: 0.412183\n(train @ 2): L: 0.945246; A: 0.482456; R: 0.482456; P: 0.561379; F1: 0.449099\n(valid @ 2): L: 0.998492; A: 0.451754; R: 0.451754; P: 0.560523; F1: 0.382529\n(train @ 3): L: 0.912040; A: 0.493421; R: 0.493421; P: 0.569558; F1: 0.464476\n(valid @ 3): L: 1.250378; A: 0.491228; R: 0.491228; P: 0.525911; F1: 0.476556\n(train @ 4): L: 0.975016; A: 0.565789; R: 0.565789; P: 0.593927; F1: 0.559378\n(valid @ 4): L: 0.975711; A: 0.526316; R: 0.526316; P: 0.608895; F1: 0.494469\n(train @ 5): L: 0.816230; A: 0.616228; R: 0.616228; P: 0.627962; F1: 0.610786\n(valid @ 5): L: 0.883267; A: 0.627193; R: 0.627193; P: 0.655977; F1: 0.630118\n(train @ 6): L: 0.770158; A: 0.679825; R: 0.679825; P: 0.714616; F1: 0.674596\n(valid @ 6): L: 1.025684; A: 0.482456; R: 0.482456; P: 0.594568; F1: 0.424975\n(train @ 7): L: 0.869398; A: 0.524123; R: 0.524123; P: 0.540238; F1: 0.527220\n(valid @ 7): L: 1.206449; A: 0.561404; R: 0.561404; P: 0.598086; F1: 0.533835\n(train @ 8): L: 0.915536; A: 0.576754; R: 0.576754; P: 0.577679; F1: 0.576584\n(valid @ 8): L: 0.966880; A: 0.469298; R: 0.469298; P: 0.765220; F1: 0.335613\n(train @ 9): L: 0.824403; A: 0.543860; R: 0.543860; P: 0.550845; F1: 0.545533\n(valid @ 9): L: 0.848068; A: 0.649123; R: 0.649123; P: 0.651634; F1: 0.643673\n(train @ 10): L: 0.738878; A: 0.664474; R: 0.664474; P: 0.665795; F1: 0.663683\n(valid @ 10): L: 0.878918; A: 0.552632; R: 0.552632; P: 0.566158; F1: 0.485874\n(train @ 11): L: 0.738103; A: 0.572368; R: 0.572368; P: 0.577047; F1: 0.573897\n(valid @ 11): L: 0.733031; A: 0.666667; R: 0.666667; P: 0.665864; F1: 0.665366\n(train @ 12): L: 0.625226; A: 0.728070; R: 0.728070; P: 0.746197; F1: 0.728354\n(valid @ 12): L: 0.761645; A: 0.627193; R: 0.627193; P: 0.636683; F1: 0.619439\n(train @ 13): L: 0.633128; A: 0.686404; R: 0.686404; P: 0.690656; F1: 0.686956\n(valid @ 13): L: 0.953445; A: 0.614035; R: 0.614035; P: 0.644528; F1: 0.593868\n(train @ 14): L: 0.732491; A: 0.657895; R: 0.657895; P: 0.659091; F1: 0.658130\n(valid @ 14): L: 0.887071; A: 0.600877; R: 0.600877; P: 0.604106; F1: 0.582563\n(train @ 15): L: 0.750678; A: 0.589912; R: 0.589912; P: 0.589181; F1: 0.586655\n(valid @ 15): L: 1.024972; A: 0.561404; R: 0.561404; P: 0.596023; F1: 0.535469\n(train @ 16): L: 0.846797; A: 0.574561; R: 0.574561; P: 0.571551; F1: 0.571096\n(valid @ 16): L: 0.705839; A: 0.675439; R: 0.675439; P: 0.701063; F1: 0.668037\n(train @ 17): L: 0.661203; A: 0.684211; R: 0.684211; P: 0.698582; F1: 0.682518\n(valid @ 17): L: 0.716377; A: 0.640351; R: 0.640351; P: 0.706215; F1: 0.607625\n(train @ 18): L: 0.682743; A: 0.690789; R: 0.690789; P: 0.731616; F1: 0.690801\n(valid @ 18): L: 0.793429; A: 0.666667; R: 0.666667; P: 0.735774; F1: 0.646966\n(train @ 19): L: 0.625881; A: 0.752193; R: 0.752193; P: 0.770286; F1: 0.754720\n(valid @ 19): L: 0.668152; A: 0.692982; R: 0.692982; P: 0.703796; F1: 0.693556\n(train @ 20): L: 0.555781; A: 0.730263; R: 0.730263; P: 0.733387; F1: 0.729698\n(valid @ 20): L: 0.705547; A: 0.666667; R: 0.666667; P: 0.674496; F1: 0.660428\n(train @ 21): L: 0.495258; A: 0.791667; R: 0.791667; P: 0.801994; F1: 0.792160\n(valid @ 21): L: 0.692874; A: 0.701754; R: 0.701754; P: 0.710954; F1: 0.700356\n(train @ 22): L: 0.517639; A: 0.780702; R: 0.780702; P: 0.784895; F1: 0.780931\n(valid @ 22): L: 0.697780; A: 0.649123; R: 0.649123; P: 0.669769; F1: 0.637517\n(train @ 23): L: 0.516263; A: 0.750000; R: 0.750000; P: 0.749733; F1: 0.749794\n(valid @ 23): L: 0.727997; A: 0.622807; R: 0.622807; P: 0.641164; F1: 0.607455\n(train @ 24): L: 0.450271; A: 0.815789; R: 0.815789; P: 0.819883; F1: 0.815256\n(valid @ 24): L: 0.716183; A: 0.697368; R: 0.697368; P: 0.730923; F1: 0.692023\n(train @ 25): L: 0.411158; A: 0.833333; R: 0.833333; P: 0.836422; F1: 0.833508\n(valid @ 25): L: 0.813549; A: 0.653509; R: 0.653509; P: 0.672103; F1: 0.638276\n(train @ 26): L: 0.491303; A: 0.771930; R: 0.771930; P: 0.775466; F1: 0.771328\n(valid @ 26): L: 0.639806; A: 0.697368; R: 0.697368; P: 0.701750; F1: 0.696628\n(train @ 27): L: 0.410792; A: 0.842105; R: 0.842105; P: 0.843906; F1: 0.842227\n(valid @ 27): L: 0.722009; A: 0.675439; R: 0.675439; P: 0.674637; F1: 0.672107\n(train @ 28): L: 0.364842; A: 0.837719; R: 0.837719; P: 0.847286; F1: 0.838229\n(valid @ 28): L: 0.956086; A: 0.605263; R: 0.605263; P: 0.683153; F1: 0.569463\n(train @ 29): L: 0.377716; A: 0.833333; R: 0.833333; P: 0.838259; F1: 0.833997\n(valid @ 29): L: 0.726179; A: 0.714912; R: 0.714912; P: 0.739452; F1: 0.717570\n(train @ 30): L: 0.324321; A: 0.861842; R: 0.861842; P: 0.864596; F1: 0.861941\n(valid @ 30): L: 0.788355; A: 0.671053; R: 0.671053; P: 0.706219; F1: 0.660922\nBest val Metric 0.717570 @ 29\n\n{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'test': [10, 11, 12]}\n(train @ 1): L: 1.301895; A: 0.438596; R: 0.438596; P: 0.429551; F1: 0.433183\n(valid @ 1): L: 1.013824; A: 0.504386; R: 0.504386; P: 0.460449; F1: 0.447305\n(train @ 2): L: 1.027758; A: 0.447368; R: 0.447368; P: 0.376886; F1: 0.408407\n(valid @ 2): L: 0.908666; A: 0.491228; R: 0.491228; P: 0.603283; F1: 0.435783\n(train @ 3): L: 0.936614; A: 0.458333; R: 0.458333; P: 0.513744; F1: 0.432105\n(valid @ 3): L: 0.962074; A: 0.609649; R: 0.609649; P: 0.663570; F1: 0.604208\n(train @ 4): L: 0.988010; A: 0.524123; R: 0.524123; P: 0.553600; F1: 0.513818\n(valid @ 4): L: 1.139749; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n(train @ 5): L: 0.869700; A: 0.537281; R: 0.537281; P: 0.540217; F1: 0.531868\n(valid @ 5): L: 0.874917; A: 0.570175; R: 0.570175; P: 0.787303; F1: 0.492115\n(train @ 6): L: 0.829537; A: 0.578947; R: 0.578947; P: 0.585625; F1: 0.574587\n(valid @ 6): L: 0.846998; A: 0.574561; R: 0.574561; P: 0.641098; F1: 0.560248\n(train @ 7): L: 0.885264; A: 0.524123; R: 0.524123; P: 0.528578; F1: 0.525423\n(valid @ 7): L: 0.862722; A: 0.557018; R: 0.557018; P: 0.611308; F1: 0.537246\n(train @ 8): L: 0.873105; A: 0.561404; R: 0.561404; P: 0.565594; F1: 0.555213\n(valid @ 8): L: 1.194860; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n(train @ 9): L: 0.943991; A: 0.484649; R: 0.484649; P: 0.488265; F1: 0.484220\n(valid @ 9): L: 0.787683; A: 0.565789; R: 0.565789; P: 0.703935; F1: 0.475099\n(train @ 10): L: 0.793125; A: 0.557018; R: 0.557018; P: 0.557744; F1: 0.554714\n(valid @ 10): L: 0.804448; A: 0.535088; R: 0.535088; P: 0.689630; F1: 0.455665\n(train @ 11): L: 0.765455; A: 0.563596; R: 0.563596; P: 0.561069; F1: 0.560767\n(valid @ 11): L: 0.755720; A: 0.605263; R: 0.605263; P: 0.666667; F1: 0.568798\n(train @ 12): L: 0.684962; A: 0.666667; R: 0.666667; P: 0.674251; F1: 0.666748\n(valid @ 12): L: 0.774505; A: 0.583333; R: 0.583333; P: 0.743124; F1: 0.498645\n(train @ 13): L: 0.698410; A: 0.666667; R: 0.666667; P: 0.671241; F1: 0.667352\n(valid @ 13): L: 0.789138; A: 0.596491; R: 0.596491; P: 0.762468; F1: 0.509488\n(train @ 14): L: 0.770372; A: 0.594298; R: 0.594298; P: 0.596052; F1: 0.592366\n(valid @ 14): L: 0.737545; A: 0.684211; R: 0.684211; P: 0.715392; F1: 0.684741\n(train @ 15): L: 0.782423; A: 0.589912; R: 0.589912; P: 0.588971; F1: 0.587272\n(valid @ 15): L: 0.774802; A: 0.605263; R: 0.605263; P: 0.697842; F1: 0.584005\n(train @ 16): L: 0.788267; A: 0.539474; R: 0.539474; P: 0.539955; F1: 0.538504\n(valid @ 16): L: 0.705279; A: 0.688596; R: 0.688596; P: 0.778204; F1: 0.676539\n(train @ 17): L: 0.685280; A: 0.675439; R: 0.675439; P: 0.691834; F1: 0.673904\n(valid @ 17): L: 0.626811; A: 0.745614; R: 0.745614; P: 0.794924; F1: 0.744771\n(train @ 18): L: 0.671684; A: 0.690789; R: 0.690789; P: 0.718024; F1: 0.692634\n(valid @ 18): L: 0.591396; A: 0.754386; R: 0.754386; P: 0.761030; F1: 0.753826\n(train @ 19): L: 0.677188; A: 0.699561; R: 0.699561; P: 0.708241; F1: 0.700730\n(valid @ 19): L: 0.626528; A: 0.714912; R: 0.714912; P: 0.776813; F1: 0.712229\n(train @ 20): L: 0.592648; A: 0.732456; R: 0.732456; P: 0.742494; F1: 0.731789\n(valid @ 20): L: 0.622718; A: 0.719298; R: 0.719298; P: 0.739914; F1: 0.721970\n(train @ 21): L: 0.577414; A: 0.730263; R: 0.730263; P: 0.738305; F1: 0.729502\n(valid @ 21): L: 0.542145; A: 0.771930; R: 0.771930; P: 0.788155; F1: 0.773922\n(train @ 22): L: 0.563592; A: 0.765351; R: 0.765351; P: 0.767719; F1: 0.764969\n(valid @ 22): L: 0.661534; A: 0.671053; R: 0.671053; P: 0.743377; F1: 0.657216\n(train @ 23): L: 0.615297; A: 0.690789; R: 0.690789; P: 0.689311; F1: 0.689747\n(valid @ 23): L: 0.795266; A: 0.649123; R: 0.649123; P: 0.739639; F1: 0.631892\n(train @ 24): L: 0.535322; A: 0.752193; R: 0.752193; P: 0.753068; F1: 0.751889\n(valid @ 24): L: 0.540217; A: 0.767544; R: 0.767544; P: 0.790747; F1: 0.768490\n(train @ 25): L: 0.482843; A: 0.809211; R: 0.809211; P: 0.811965; F1: 0.809805\n(valid @ 25): L: 0.695504; A: 0.653509; R: 0.653509; P: 0.731016; F1: 0.627161\n(train @ 26): L: 0.536420; A: 0.760965; R: 0.760965; P: 0.764899; F1: 0.761031\n(valid @ 26): L: 0.518378; A: 0.763158; R: 0.763158; P: 0.795898; F1: 0.763127\n(train @ 27): L: 0.544617; A: 0.767544; R: 0.767544; P: 0.780077; F1: 0.769872\n(valid @ 27): L: 0.651974; A: 0.736842; R: 0.736842; P: 0.795489; F1: 0.733790\n(train @ 28): L: 0.465277; A: 0.778509; R: 0.778509; P: 0.785290; F1: 0.778205\n(valid @ 28): L: 0.554163; A: 0.714912; R: 0.714912; P: 0.729318; F1: 0.717677\n(train @ 29): L: 0.464071; A: 0.793860; R: 0.793860; P: 0.798153; F1: 0.794643\n(valid @ 29): L: 0.517687; A: 0.745614; R: 0.745614; P: 0.785370; F1: 0.740142\n(train @ 30): L: 0.407575; A: 0.811404; R: 0.811404; P: 0.813043; F1: 0.811440\n(valid @ 30): L: 0.512614; A: 0.750000; R: 0.750000; P: 0.786883; F1: 0.750075\nBest val Metric 0.773922 @ 21\n\n{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'test': [10, 11, 12]}\n(train @ 1): L: 1.377982; A: 0.440789; R: 0.440789; P: 0.437169; F1: 0.438407\n(valid @ 1): L: 1.057246; A: 0.592105; R: 0.592105; P: 0.655556; F1: 0.553748\n(train @ 2): L: 1.021019; A: 0.449561; R: 0.449561; P: 0.450246; F1: 0.425644\n(valid @ 2): L: 0.975680; A: 0.438596; R: 0.438596; P: 0.338075; F1: 0.290083\n(train @ 3): L: 0.939833; A: 0.497807; R: 0.497807; P: 0.572891; F1: 0.465497\n(valid @ 3): L: 0.951345; A: 0.600877; R: 0.600877; P: 0.650751; F1: 0.593477\n(train @ 4): L: 0.933772; A: 0.539474; R: 0.539474; P: 0.604842; F1: 0.507084\n(valid @ 4): L: 1.217686; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n(train @ 5): L: 0.947217; A: 0.497807; R: 0.497807; P: 0.508636; F1: 0.484260\n(valid @ 5): L: 0.904597; A: 0.622807; R: 0.622807; P: 0.643620; F1: 0.603689\n(train @ 6): L: 0.894864; A: 0.517544; R: 0.517544; P: 0.516568; F1: 0.511595\n(valid @ 6): L: 0.947655; A: 0.535088; R: 0.535088; P: 0.629321; F1: 0.503307\n(train @ 7): L: 0.923316; A: 0.528509; R: 0.528509; P: 0.529581; F1: 0.528468\n(valid @ 7): L: 0.940572; A: 0.486842; R: 0.486842; P: 0.580893; F1: 0.443707\n(train @ 8): L: 0.902744; A: 0.526316; R: 0.526316; P: 0.533870; F1: 0.517490\n(valid @ 8): L: 1.195367; A: 0.416667; R: 0.416667; P: 0.176211; F1: 0.247678\n(train @ 9): L: 0.941397; A: 0.510965; R: 0.510965; P: 0.520207; F1: 0.506516\n(valid @ 9): L: 0.854287; A: 0.657895; R: 0.657895; P: 0.780651; F1: 0.603048\n(train @ 10): L: 0.847215; A: 0.557018; R: 0.557018; P: 0.577209; F1: 0.544681\n(valid @ 10): L: 0.902432; A: 0.482456; R: 0.482456; P: 0.587203; F1: 0.428425\n(train @ 11): L: 0.769256; A: 0.572368; R: 0.572368; P: 0.579661; F1: 0.569156\n(valid @ 11): L: 0.930880; A: 0.508772; R: 0.508772; P: 0.678957; F1: 0.414486\n(train @ 12): L: 0.746593; A: 0.583333; R: 0.583333; P: 0.594255; F1: 0.583595\n(valid @ 12): L: 0.897945; A: 0.526316; R: 0.526316; P: 0.356037; F1: 0.395789\n(train @ 13): L: 0.764677; A: 0.620614; R: 0.620614; P: 0.629416; F1: 0.621291\n(valid @ 13): L: 0.859915; A: 0.596491; R: 0.596491; P: 0.793953; F1: 0.540129\n(train @ 14): L: 0.741103; A: 0.609649; R: 0.609649; P: 0.617097; F1: 0.605950\n(valid @ 14): L: 0.900401; A: 0.609649; R: 0.609649; P: 0.666535; F1: 0.600663\n(train @ 15): L: 0.725685; A: 0.587719; R: 0.587719; P: 0.591148; F1: 0.584549\n(valid @ 15): L: 0.736241; A: 0.605263; R: 0.605263; P: 0.651248; F1: 0.594755\n(train @ 16): L: 0.753138; A: 0.550439; R: 0.550439; P: 0.550807; F1: 0.549889\n(valid @ 16): L: 0.792264; A: 0.657895; R: 0.657895; P: 0.739943; F1: 0.642300\n(train @ 17): L: 0.669185; A: 0.664474; R: 0.664474; P: 0.675717; F1: 0.660887\n(valid @ 17): L: 0.710851; A: 0.697368; R: 0.697368; P: 0.736801; F1: 0.696629\n(train @ 18): L: 0.671767; A: 0.677632; R: 0.677632; P: 0.696606; F1: 0.675052\n(valid @ 18): L: 0.639589; A: 0.750000; R: 0.750000; P: 0.764757; F1: 0.750570\n(train @ 19): L: 0.639242; A: 0.719298; R: 0.719298; P: 0.727438; F1: 0.719395\n(valid @ 19): L: 0.669512; A: 0.706140; R: 0.706140; P: 0.776008; F1: 0.700645\n(train @ 20): L: 0.582668; A: 0.752193; R: 0.752193; P: 0.755103; F1: 0.751590\n(valid @ 20): L: 0.637833; A: 0.741228; R: 0.741228; P: 0.759746; F1: 0.741974\n(train @ 21): L: 0.590702; A: 0.714912; R: 0.714912; P: 0.720973; F1: 0.714567\n(valid @ 21): L: 0.616579; A: 0.706140; R: 0.706140; P: 0.775056; F1: 0.700585\n(train @ 22): L: 0.625819; A: 0.666667; R: 0.666667; P: 0.667060; F1: 0.665194\n(valid @ 22): L: 0.820628; A: 0.570175; R: 0.570175; P: 0.683047; F1: 0.525314\n(train @ 23): L: 0.668496; A: 0.655702; R: 0.655702; P: 0.659711; F1: 0.655731\n(valid @ 23): L: 0.891406; A: 0.561404; R: 0.561404; P: 0.680759; F1: 0.515552\n(train @ 24): L: 0.598336; A: 0.695175; R: 0.695175; P: 0.696584; F1: 0.695067\n(valid @ 24): L: 0.574932; A: 0.736842; R: 0.736842; P: 0.756771; F1: 0.738639\n(train @ 25): L: 0.535388; A: 0.763158; R: 0.763158; P: 0.772541; F1: 0.763513\n(valid @ 25): L: 0.649807; A: 0.666667; R: 0.666667; P: 0.736774; F1: 0.648164\n(train @ 26): L: 0.665631; A: 0.703947; R: 0.703947; P: 0.706584; F1: 0.702317\n(valid @ 26): L: 0.668865; A: 0.750000; R: 0.750000; P: 0.775557; F1: 0.746760\n(train @ 27): L: 0.579283; A: 0.734649; R: 0.734649; P: 0.752564; F1: 0.734203\n(valid @ 27): L: 0.761145; A: 0.692982; R: 0.692982; P: 0.759742; F1: 0.683282\n(train @ 28): L: 0.551579; A: 0.732456; R: 0.732456; P: 0.740355; F1: 0.732680\n(valid @ 28): L: 0.554384; A: 0.728070; R: 0.728070; P: 0.761678; F1: 0.727389\n(train @ 29): L: 0.486401; A: 0.785088; R: 0.785088; P: 0.793712; F1: 0.785075\n(valid @ 29): L: 0.589414; A: 0.697368; R: 0.697368; P: 0.738408; F1: 0.694528\n(train @ 30): L: 0.478738; A: 0.767544; R: 0.767544; P: 0.773324; F1: 0.766615\n(valid @ 30): L: 0.719634; A: 0.697368; R: 0.697368; P: 0.726151; F1: 0.693803\nBest val Metric 0.750570 @ 18\n\nAverage of Best Metrics on Each Valid Set: 0.747354, 200903214601\n"
    }
   ],
   "source": [
    "# Experiment with other parameters\n",
    "import argparse\n",
    "from main import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}