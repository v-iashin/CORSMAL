{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reproduce our experiments just run the first cell. Only GPU back-end is supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "init_time: 200903163404\n    task: ftype\n    output_dim: 4\n    model_type: GRU\n    bi_dir: False\n    device: cuda:0\n    data_root: ../../filling_level/vggish/vggish_features\n    batch_size: 64\n    input_dim: 128\n    hidden_dim: 512\n    n_layers: 5\n    drop_p: 0.0\n    num_epochs: 30\n    seed: 1337\n{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'test': [10, 11, 12]}\n(train @ 1): L: 1.435528; A: 0.296053; R: 0.296053; P: 0.297282; F1: 0.284392\n(valid @ 1): L: 1.403968; A: 0.346491; R: 0.346491; P: 0.274202; F1: 0.246877\n(train @ 2): L: 1.355512; A: 0.326754; R: 0.326754; P: 0.265790; F1: 0.288936\n(valid @ 2): L: 1.347115; A: 0.385965; R: 0.385965; P: 0.182825; F1: 0.248120\n(train @ 3): L: 1.338570; A: 0.350877; R: 0.350877; P: 0.280349; F1: 0.308697\n(valid @ 3): L: 1.297946; A: 0.412281; R: 0.412281; P: 0.307723; F1: 0.324470\n(train @ 4): L: 1.328715; A: 0.326754; R: 0.326754; P: 0.335725; F1: 0.304643\n(valid @ 4): L: 1.268678; A: 0.315789; R: 0.315789; P: 0.099723; F1: 0.151579\n(train @ 5): L: 1.246402; A: 0.377193; R: 0.377193; P: 0.397946; F1: 0.356769\n(valid @ 5): L: 1.115692; A: 0.504386; R: 0.504386; P: 0.527962; F1: 0.480276\n(train @ 6): L: 1.187328; A: 0.388158; R: 0.388158; P: 0.398701; F1: 0.376050\n(valid @ 6): L: 1.118171; A: 0.425439; R: 0.425439; P: 0.364662; F1: 0.368563\n(train @ 7): L: 1.189692; A: 0.383772; R: 0.383772; P: 0.374229; F1: 0.348271\n(valid @ 7): L: 1.123886; A: 0.521930; R: 0.521930; P: 0.596201; F1: 0.487944\n(train @ 8): L: 1.044008; A: 0.489035; R: 0.489035; P: 0.478644; F1: 0.479735\n(valid @ 8): L: 0.885678; A: 0.723684; R: 0.723684; P: 0.745728; F1: 0.724340\n(train @ 9): L: 0.858133; A: 0.642544; R: 0.642544; P: 0.692780; F1: 0.618590\n(valid @ 9): L: 0.732799; A: 0.692982; R: 0.692982; P: 0.715715; F1: 0.670914\n(train @ 10): L: 0.699228; A: 0.714912; R: 0.714912; P: 0.737887; F1: 0.687563\n(valid @ 10): L: 0.663705; A: 0.750000; R: 0.750000; P: 0.774644; F1: 0.747244\n(train @ 11): L: 0.768314; A: 0.699561; R: 0.699561; P: 0.694733; F1: 0.688113\n(valid @ 11): L: 0.428039; A: 0.815789; R: 0.815789; P: 0.813404; F1: 0.813249\n(train @ 12): L: 0.426726; A: 0.809211; R: 0.809211; P: 0.808940; F1: 0.803345\n(valid @ 12): L: 0.522549; A: 0.754386; R: 0.754386; P: 0.768031; F1: 0.753524\n(train @ 13): L: 0.518313; A: 0.802632; R: 0.802632; P: 0.801180; F1: 0.800668\n(valid @ 13): L: 0.299965; A: 0.842105; R: 0.842105; P: 0.842538; F1: 0.838935\n(train @ 14): L: 0.383844; A: 0.833333; R: 0.833333; P: 0.836189; F1: 0.830651\n(valid @ 14): L: 0.359081; A: 0.868421; R: 0.868421; P: 0.897771; F1: 0.869235\n(train @ 15): L: 0.298331; A: 0.866228; R: 0.866228; P: 0.869331; F1: 0.864437\n(valid @ 15): L: 0.240245; A: 0.881579; R: 0.881579; P: 0.893559; F1: 0.876506\n(train @ 16): L: 0.242944; A: 0.896930; R: 0.896930; P: 0.903873; F1: 0.894593\n(valid @ 16): L: 0.253270; A: 0.885965; R: 0.885965; P: 0.890694; F1: 0.886409\n(train @ 17): L: 0.225206; A: 0.910088; R: 0.910088; P: 0.910321; F1: 0.910196\n(valid @ 17): L: 0.234423; A: 0.868421; R: 0.868421; P: 0.871483; F1: 0.865992\n(train @ 18): L: 0.318216; A: 0.857456; R: 0.857456; P: 0.856543; F1: 0.856242\n(valid @ 18): L: 0.331429; A: 0.885965; R: 0.885965; P: 0.912460; F1: 0.885173\n(train @ 19): L: 0.248512; A: 0.896930; R: 0.896930; P: 0.899686; F1: 0.895773\n(valid @ 19): L: 0.276882; A: 0.881579; R: 0.881579; P: 0.883985; F1: 0.881623\n(train @ 20): L: 0.357652; A: 0.870614; R: 0.870614; P: 0.884050; F1: 0.871767\n(valid @ 20): L: 0.281950; A: 0.877193; R: 0.877193; P: 0.882451; F1: 0.875078\n(train @ 21): L: 0.246157; A: 0.901316; R: 0.901316; P: 0.908990; F1: 0.898456\n(valid @ 21): L: 0.337033; A: 0.859649; R: 0.859649; P: 0.864184; F1: 0.860899\n(train @ 22): L: 0.199422; A: 0.921053; R: 0.921053; P: 0.922355; F1: 0.921369\n(valid @ 22): L: 0.325866; A: 0.846491; R: 0.846491; P: 0.846847; F1: 0.846595\n(train @ 23): L: 0.188134; A: 0.934211; R: 0.934211; P: 0.934821; F1: 0.933917\n(valid @ 23): L: 0.217522; A: 0.894737; R: 0.894737; P: 0.897948; F1: 0.895293\n(train @ 24): L: 0.164524; A: 0.925439; R: 0.925439; P: 0.931102; F1: 0.925224\n(valid @ 24): L: 0.192815; A: 0.907895; R: 0.907895; P: 0.918726; F1: 0.907898\n(train @ 25): L: 0.205490; A: 0.905702; R: 0.905702; P: 0.907097; F1: 0.905454\n(valid @ 25): L: 0.194571; A: 0.903509; R: 0.903509; P: 0.907358; F1: 0.904032\n(train @ 26): L: 0.177724; A: 0.916667; R: 0.916667; P: 0.926218; F1: 0.915635\n(valid @ 26): L: 0.371094; A: 0.833333; R: 0.833333; P: 0.836439; F1: 0.833543\n(train @ 27): L: 0.174794; A: 0.940789; R: 0.940789; P: 0.940429; F1: 0.940528\n(valid @ 27): L: 0.219663; A: 0.916667; R: 0.916667; P: 0.929165; F1: 0.916710\n(train @ 28): L: 0.158117; A: 0.932018; R: 0.932018; P: 0.933716; F1: 0.931822\n(valid @ 28): L: 0.243568; A: 0.877193; R: 0.877193; P: 0.877587; F1: 0.876986\n(train @ 29): L: 0.144643; A: 0.934211; R: 0.934211; P: 0.933975; F1: 0.933939\n(valid @ 29): L: 0.179761; A: 0.916667; R: 0.916667; P: 0.926833; F1: 0.916359\n(train @ 30): L: 0.137558; A: 0.934211; R: 0.934211; P: 0.934706; F1: 0.934121\n(valid @ 30): L: 0.256669; A: 0.899123; R: 0.899123; P: 0.902945; F1: 0.900016\nBest val Metric 0.916710 @ 27\n\nmodel is saved @ ./predictions/200903163404_200910222303/ftype_1_2_4_5_7_8_pretrained_model.pt\nSaving predictions @ ./predictions/200903163404_200910222303/ftype_train_1_2_4_5_7_8_vggish.csv\nSaving predictions @ ./predictions/200903163404_200910222303/ftype_valid_3_6_9_vggish.csv\nSaving predictions @ ./predictions/200903163404_200910222303/ftype_test_trained_on_1_2_4_5_7_8_vggish.csv\n{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'test': [10, 11, 12]}\n(train @ 1): L: 1.463795; A: 0.311404; R: 0.311404; P: 0.314404; F1: 0.303712\n(valid @ 1): L: 1.412057; A: 0.364035; R: 0.364035; P: 0.309656; F1: 0.261692\n(train @ 2): L: 1.354274; A: 0.348684; R: 0.348684; P: 0.433198; F1: 0.272816\n(valid @ 2): L: 1.311451; A: 0.394737; R: 0.394737; P: 0.272161; F1: 0.313122\n(train @ 3): L: 1.343737; A: 0.320175; R: 0.320175; P: 0.245301; F1: 0.250675\n(valid @ 3): L: 1.311615; A: 0.403509; R: 0.403509; P: 0.297171; F1: 0.316206\n(train @ 4): L: 1.378471; A: 0.326754; R: 0.326754; P: 0.330725; F1: 0.297706\n(valid @ 4): L: 1.256554; A: 0.315789; R: 0.315789; P: 0.099723; F1: 0.151579\n(train @ 5): L: 1.271290; A: 0.353070; R: 0.353070; P: 0.378490; F1: 0.338063\n(valid @ 5): L: 1.195513; A: 0.372807; R: 0.372807; P: 0.364185; F1: 0.299166\n(train @ 6): L: 1.190326; A: 0.368421; R: 0.368421; P: 0.369884; F1: 0.363059\n(valid @ 6): L: 1.248829; A: 0.399123; R: 0.399123; P: 0.381564; F1: 0.345276\n(train @ 7): L: 1.233919; A: 0.359649; R: 0.359649; P: 0.357302; F1: 0.342710\n(valid @ 7): L: 1.152015; A: 0.407895; R: 0.407895; P: 0.405196; F1: 0.350973\n(train @ 8): L: 1.146071; A: 0.403509; R: 0.403509; P: 0.405825; F1: 0.394942\n(valid @ 8): L: 1.022720; A: 0.530702; R: 0.530702; P: 0.556975; F1: 0.505094\n(train @ 9): L: 1.085386; A: 0.541667; R: 0.541667; P: 0.565635; F1: 0.520026\n(valid @ 9): L: 0.889048; A: 0.679825; R: 0.679825; P: 0.729899; F1: 0.660792\n(train @ 10): L: 0.865110; A: 0.618421; R: 0.618421; P: 0.654734; F1: 0.592891\n(valid @ 10): L: 0.665477; A: 0.728070; R: 0.728070; P: 0.728365; F1: 0.727367\n(train @ 11): L: 0.602247; A: 0.732456; R: 0.732456; P: 0.735334; F1: 0.728180\n(valid @ 11): L: 0.511342; A: 0.785088; R: 0.785088; P: 0.810636; F1: 0.773718\n(train @ 12): L: 0.549570; A: 0.750000; R: 0.750000; P: 0.750697; F1: 0.744400\n(valid @ 12): L: 0.565023; A: 0.754386; R: 0.754386; P: 0.779347; F1: 0.747915\n(train @ 13): L: 0.486687; A: 0.804825; R: 0.804825; P: 0.804552; F1: 0.801648\n(valid @ 13): L: 0.337066; A: 0.859649; R: 0.859649; P: 0.869495; F1: 0.855506\n(train @ 14): L: 0.352523; A: 0.868421; R: 0.868421; P: 0.871014; F1: 0.866569\n(valid @ 14): L: 0.310246; A: 0.850877; R: 0.850877; P: 0.876090; F1: 0.844387\n(train @ 15): L: 0.329374; A: 0.877193; R: 0.877193; P: 0.880282; F1: 0.875886\n(valid @ 15): L: 0.349508; A: 0.846491; R: 0.846491; P: 0.875405; F1: 0.837407\n(train @ 16): L: 0.282384; A: 0.870614; R: 0.870614; P: 0.881414; F1: 0.869538\n(valid @ 16): L: 0.310440; A: 0.859649; R: 0.859649; P: 0.883485; F1: 0.852602\n(train @ 17): L: 0.242681; A: 0.896930; R: 0.896930; P: 0.896492; F1: 0.895686\n(valid @ 17): L: 0.331758; A: 0.868421; R: 0.868421; P: 0.887004; F1: 0.862999\n(train @ 18): L: 0.250162; A: 0.881579; R: 0.881579; P: 0.880975; F1: 0.880526\n(valid @ 18): L: 0.291591; A: 0.864035; R: 0.864035; P: 0.867569; F1: 0.859409\n(train @ 19): L: 0.240980; A: 0.892544; R: 0.892544; P: 0.893139; F1: 0.892529\n(valid @ 19): L: 0.213920; A: 0.899123; R: 0.899123; P: 0.908767; F1: 0.897575\n(train @ 20): L: 0.215412; A: 0.912281; R: 0.912281; P: 0.912660; F1: 0.912132\n(valid @ 20): L: 0.249179; A: 0.899123; R: 0.899123; P: 0.899348; F1: 0.897892\n(train @ 21): L: 0.197138; A: 0.910088; R: 0.910088; P: 0.914615; F1: 0.908945\n(valid @ 21): L: 0.209100; A: 0.885965; R: 0.885965; P: 0.889885; F1: 0.884429\n(train @ 22): L: 0.185541; A: 0.923246; R: 0.923246; P: 0.923313; F1: 0.923171\n(valid @ 22): L: 0.240815; A: 0.885965; R: 0.885965; P: 0.903683; F1: 0.883723\n(train @ 23): L: 0.135638; A: 0.949561; R: 0.949561; P: 0.949893; F1: 0.949445\n(valid @ 23): L: 0.178980; A: 0.899123; R: 0.899123; P: 0.909589; F1: 0.896825\n(train @ 24): L: 0.128024; A: 0.942982; R: 0.942982; P: 0.943069; F1: 0.942966\n(valid @ 24): L: 0.165661; A: 0.907895; R: 0.907895; P: 0.916167; F1: 0.907188\n(train @ 25): L: 0.106492; A: 0.953947; R: 0.953947; P: 0.955288; F1: 0.953892\n(valid @ 25): L: 0.167406; A: 0.899123; R: 0.899123; P: 0.903295; F1: 0.898870\n(train @ 26): L: 0.116411; A: 0.962719; R: 0.962719; P: 0.963611; F1: 0.962690\n(valid @ 26): L: 0.171339; A: 0.894737; R: 0.894737; P: 0.899667; F1: 0.894366\n(train @ 27): L: 0.096510; A: 0.967105; R: 0.967105; P: 0.967348; F1: 0.967097\n(valid @ 27): L: 0.191102; A: 0.916667; R: 0.916667; P: 0.931100; F1: 0.915452\n(train @ 28): L: 0.130013; A: 0.942982; R: 0.942982; P: 0.944365; F1: 0.943031\n(valid @ 28): L: 0.179075; A: 0.894737; R: 0.894737; P: 0.900125; F1: 0.893663\n(train @ 29): L: 0.108975; A: 0.953947; R: 0.953947; P: 0.953910; F1: 0.953881\n(valid @ 29): L: 0.195180; A: 0.894737; R: 0.894737; P: 0.903535; F1: 0.893946\n(train @ 30): L: 0.158901; A: 0.940789; R: 0.940789; P: 0.941655; F1: 0.941093\n(valid @ 30): L: 0.180264; A: 0.903509; R: 0.903509; P: 0.905090; F1: 0.902039\nBest val Metric 0.915452 @ 27\n\nmodel is saved @ ./predictions/200903163404_200910222303/ftype_1_3_4_6_7_9_pretrained_model.pt\nSaving predictions @ ./predictions/200903163404_200910222303/ftype_train_1_3_4_6_7_9_vggish.csv\nSaving predictions @ ./predictions/200903163404_200910222303/ftype_valid_2_5_8_vggish.csv\nSaving predictions @ ./predictions/200903163404_200910222303/ftype_test_trained_on_1_3_4_6_7_9_vggish.csv\n{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'test': [10, 11, 12]}\n(train @ 1): L: 1.447026; A: 0.315789; R: 0.315789; P: 0.320199; F1: 0.308581\n(valid @ 1): L: 1.437785; A: 0.333333; R: 0.333333; P: 0.249521; F1: 0.243581\n(train @ 2): L: 1.335825; A: 0.326754; R: 0.326754; P: 0.208503; F1: 0.249127\n(valid @ 2): L: 1.343691; A: 0.350877; R: 0.350877; P: 0.244304; F1: 0.273128\n(train @ 3): L: 1.289269; A: 0.339912; R: 0.339912; P: 0.342170; F1: 0.303089\n(valid @ 3): L: 1.355668; A: 0.372807; R: 0.372807; P: 0.330662; F1: 0.335993\n(train @ 4): L: 1.269640; A: 0.339912; R: 0.339912; P: 0.411708; F1: 0.321450\n(valid @ 4): L: 1.359984; A: 0.355263; R: 0.355263; P: 0.370706; F1: 0.338587\n(train @ 5): L: 1.197034; A: 0.401316; R: 0.401316; P: 0.411601; F1: 0.379057\n(valid @ 5): L: 1.427861; A: 0.320175; R: 0.320175; P: 0.335368; F1: 0.317287\n(train @ 6): L: 1.130921; A: 0.469298; R: 0.469298; P: 0.482241; F1: 0.458017\n(valid @ 6): L: 1.215108; A: 0.416667; R: 0.416667; P: 0.445735; F1: 0.406099\n(train @ 7): L: 1.077240; A: 0.414474; R: 0.414474; P: 0.473456; F1: 0.378190\n(valid @ 7): L: 1.066176; A: 0.500000; R: 0.500000; P: 0.509313; F1: 0.477293\n(train @ 8): L: 0.905460; A: 0.614035; R: 0.614035; P: 0.629726; F1: 0.603425\n(valid @ 8): L: 0.831699; A: 0.662281; R: 0.662281; P: 0.692915; F1: 0.630428\n(train @ 9): L: 0.656599; A: 0.697368; R: 0.697368; P: 0.697249; F1: 0.666256\n(valid @ 9): L: 1.039287; A: 0.592105; R: 0.592105; P: 0.694236; F1: 0.589801\n(train @ 10): L: 0.546781; A: 0.778509; R: 0.778509; P: 0.787110; F1: 0.766976\n(valid @ 10): L: 1.106240; A: 0.491228; R: 0.491228; P: 0.655607; F1: 0.419688\n(train @ 11): L: 0.584746; A: 0.730263; R: 0.730263; P: 0.737114; F1: 0.728071\n(valid @ 11): L: 0.579667; A: 0.741228; R: 0.741228; P: 0.774240; F1: 0.723979\n(train @ 12): L: 0.324743; A: 0.857456; R: 0.857456; P: 0.857405; F1: 0.855731\n(valid @ 12): L: 0.519828; A: 0.758772; R: 0.758772; P: 0.769612; F1: 0.751043\n(train @ 13): L: 0.263122; A: 0.892544; R: 0.892544; P: 0.891989; F1: 0.891925\n(valid @ 13): L: 0.356393; A: 0.850877; R: 0.850877; P: 0.857260; F1: 0.849158\n(train @ 14): L: 0.193905; A: 0.932018; R: 0.932018; P: 0.932014; F1: 0.932009\n(valid @ 14): L: 0.337826; A: 0.842105; R: 0.842105; P: 0.846196; F1: 0.839400\n(train @ 15): L: 0.237817; A: 0.918860; R: 0.918860; P: 0.919930; F1: 0.918081\n(valid @ 15): L: 0.406647; A: 0.833333; R: 0.833333; P: 0.837913; F1: 0.832112\n(train @ 16): L: 0.191497; A: 0.916667; R: 0.916667; P: 0.923697; F1: 0.915959\n(valid @ 16): L: 0.394313; A: 0.837719; R: 0.837719; P: 0.837515; F1: 0.834019\n(train @ 17): L: 0.184927; A: 0.923246; R: 0.923246; P: 0.924311; F1: 0.922829\n(valid @ 17): L: 0.417940; A: 0.815789; R: 0.815789; P: 0.827409; F1: 0.812110\n(train @ 18): L: 0.159524; A: 0.934211; R: 0.934211; P: 0.935919; F1: 0.934323\n(valid @ 18): L: 0.306650; A: 0.864035; R: 0.864035; P: 0.866043; F1: 0.861858\n(train @ 19): L: 0.133682; A: 0.947368; R: 0.947368; P: 0.947611; F1: 0.947109\n(valid @ 19): L: 0.392018; A: 0.859649; R: 0.859649; P: 0.863933; F1: 0.860381\n(train @ 20): L: 0.183692; A: 0.932018; R: 0.932018; P: 0.932660; F1: 0.932049\n(valid @ 20): L: 0.441220; A: 0.833333; R: 0.833333; P: 0.855428; F1: 0.825213\n(train @ 21): L: 0.164700; A: 0.942982; R: 0.942982; P: 0.943301; F1: 0.942913\n(valid @ 21): L: 0.284394; A: 0.859649; R: 0.859649; P: 0.865361; F1: 0.857854\n(train @ 22): L: 0.115255; A: 0.960526; R: 0.960526; P: 0.960674; F1: 0.960575\n(valid @ 22): L: 0.286154; A: 0.868421; R: 0.868421; P: 0.875739; F1: 0.865952\n(train @ 23): L: 0.079306; A: 0.971491; R: 0.971491; P: 0.971597; F1: 0.971535\n(valid @ 23): L: 0.294710; A: 0.868421; R: 0.868421; P: 0.873143; F1: 0.866096\n(train @ 24): L: 0.082918; A: 0.967105; R: 0.967105; P: 0.967528; F1: 0.967164\n(valid @ 24): L: 0.291871; A: 0.881579; R: 0.881579; P: 0.887223; F1: 0.880062\n(train @ 25): L: 0.067270; A: 0.975877; R: 0.975877; P: 0.975876; F1: 0.975869\n(valid @ 25): L: 0.301256; A: 0.885965; R: 0.885965; P: 0.900794; F1: 0.882955\n(train @ 26): L: 0.125241; A: 0.967105; R: 0.967105; P: 0.967927; F1: 0.967105\n(valid @ 26): L: 0.285843; A: 0.872807; R: 0.872807; P: 0.876757; F1: 0.871433\n(train @ 27): L: 0.052991; A: 0.980263; R: 0.980263; P: 0.980439; F1: 0.980298\n(valid @ 27): L: 0.285462; A: 0.894737; R: 0.894737; P: 0.911308; F1: 0.892227\n(train @ 28): L: 0.055603; A: 0.971491; R: 0.971491; P: 0.971965; F1: 0.971502\n(valid @ 28): L: 0.269410; A: 0.903509; R: 0.903509; P: 0.907772; F1: 0.902356\n(train @ 29): L: 0.059963; A: 0.973684; R: 0.973684; P: 0.973775; F1: 0.973720\n(valid @ 29): L: 0.244102; A: 0.899123; R: 0.899123; P: 0.905143; F1: 0.897812\n(train @ 30): L: 0.046977; A: 0.982456; R: 0.982456; P: 0.982456; F1: 0.982456\n(valid @ 30): L: 0.242103; A: 0.907895; R: 0.907895; P: 0.914300; F1: 0.906707\nBest val Metric 0.906707 @ 30\n\nmodel is saved @ ./predictions/200903163404_200910222303/ftype_2_3_5_6_8_9_pretrained_model.pt\nSaving predictions @ ./predictions/200903163404_200910222303/ftype_train_2_3_5_6_8_9_vggish.csv\nSaving predictions @ ./predictions/200903163404_200910222303/ftype_valid_1_4_7_vggish.csv\nSaving predictions @ ./predictions/200903163404_200910222303/ftype_test_trained_on_2_3_5_6_8_9_vggish.csv\nAverage of Best Metrics on Each Valid Set: 0.912957, 200903163404_200910222303\nSaved test results @ ./predictions/200903163404_200910222303/ftype_test_agg_vggish.csv\n"
    }
   ],
   "source": [
    "# Reproduce the best experiment\n",
    "import sys\n",
    "sys.path.insert(0, '../../filling_level/vggish/')  # nopep8\n",
    "import argparse\n",
    "from main import Config, run_kfold\n",
    "from time import localtime, strftime\n",
    "\n",
    "# if True, will use the pre-trained model and make predictions, if False, will train the model\n",
    "use_pretrained = True\n",
    "exp_name = 200903163404\n",
    "cfg = Config()\n",
    "cfg.load_from(path=f'./predictions/{exp_name}/cfg.txt')\n",
    "# replacing the time with the old_time + current_time such that there is no collision\n",
    "if use_pretrained:\n",
    "    cfg.init_time = exp_name\n",
    "else:\n",
    "    cfg.init_time = f'{cfg.init_time}_{strftime(\"%y%m%d%H%M%S\", localtime())}'\n",
    "run_kfold(cfg, use_pretrained)  # Expected average of Best Metrics on Each Valid Set: 0.912957 @ 200903163404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "init_time: 200903163404\n    task: ftype\n    output_dim: 4\n    model_type: GRU\n    bi_dir: False\n    device: cuda:0\n    data_root: ../../filling_level/vggish/vggish_features\n    batch_size: 64\n    input_dim: 128\n    hidden_dim: 512\n    n_layers: 5\n    drop_p: 0.0\n    num_epochs: 30\n    seed: 1337\n{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'test': [10, 11, 12]}\n(train @ 1): L: 1.435528; A: 0.296053; R: 0.296053; P: 0.297282; F1: 0.284392\n(valid @ 1): L: 1.403968; A: 0.346491; R: 0.346491; P: 0.274202; F1: 0.246877\n(train @ 2): L: 1.355512; A: 0.326754; R: 0.326754; P: 0.265790; F1: 0.288936\n(valid @ 2): L: 1.347115; A: 0.385965; R: 0.385965; P: 0.182825; F1: 0.248120\n(train @ 3): L: 1.338570; A: 0.350877; R: 0.350877; P: 0.280349; F1: 0.308697\n(valid @ 3): L: 1.297946; A: 0.412281; R: 0.412281; P: 0.307723; F1: 0.324470\n(train @ 4): L: 1.328715; A: 0.326754; R: 0.326754; P: 0.335725; F1: 0.304643\n(valid @ 4): L: 1.268678; A: 0.315789; R: 0.315789; P: 0.099723; F1: 0.151579\n(train @ 5): L: 1.246402; A: 0.377193; R: 0.377193; P: 0.397946; F1: 0.356769\n(valid @ 5): L: 1.115692; A: 0.504386; R: 0.504386; P: 0.527962; F1: 0.480276\n(train @ 6): L: 1.187328; A: 0.388158; R: 0.388158; P: 0.398701; F1: 0.376050\n(valid @ 6): L: 1.118171; A: 0.425439; R: 0.425439; P: 0.364662; F1: 0.368563\n(train @ 7): L: 1.189692; A: 0.383772; R: 0.383772; P: 0.374229; F1: 0.348271\n(valid @ 7): L: 1.123886; A: 0.521930; R: 0.521930; P: 0.596201; F1: 0.487944\n(train @ 8): L: 1.044008; A: 0.489035; R: 0.489035; P: 0.478644; F1: 0.479735\n(valid @ 8): L: 0.885678; A: 0.723684; R: 0.723684; P: 0.745728; F1: 0.724340\n(train @ 9): L: 0.858133; A: 0.642544; R: 0.642544; P: 0.692780; F1: 0.618590\n(valid @ 9): L: 0.732799; A: 0.692982; R: 0.692982; P: 0.715715; F1: 0.670914\n(train @ 10): L: 0.699228; A: 0.714912; R: 0.714912; P: 0.737887; F1: 0.687563\n(valid @ 10): L: 0.663705; A: 0.750000; R: 0.750000; P: 0.774644; F1: 0.747244\n(train @ 11): L: 0.768314; A: 0.699561; R: 0.699561; P: 0.694733; F1: 0.688113\n(valid @ 11): L: 0.428039; A: 0.815789; R: 0.815789; P: 0.813404; F1: 0.813249\n(train @ 12): L: 0.426726; A: 0.809211; R: 0.809211; P: 0.808940; F1: 0.803345\n(valid @ 12): L: 0.522549; A: 0.754386; R: 0.754386; P: 0.768031; F1: 0.753524\n(train @ 13): L: 0.518313; A: 0.802632; R: 0.802632; P: 0.801180; F1: 0.800668\n(valid @ 13): L: 0.299965; A: 0.842105; R: 0.842105; P: 0.842538; F1: 0.838935\n(train @ 14): L: 0.383844; A: 0.833333; R: 0.833333; P: 0.836189; F1: 0.830651\n(valid @ 14): L: 0.359081; A: 0.868421; R: 0.868421; P: 0.897771; F1: 0.869235\n(train @ 15): L: 0.298331; A: 0.866228; R: 0.866228; P: 0.869331; F1: 0.864437\n(valid @ 15): L: 0.240245; A: 0.881579; R: 0.881579; P: 0.893559; F1: 0.876506\n(train @ 16): L: 0.242944; A: 0.896930; R: 0.896930; P: 0.903873; F1: 0.894593\n(valid @ 16): L: 0.253270; A: 0.885965; R: 0.885965; P: 0.890694; F1: 0.886409\n(train @ 17): L: 0.225206; A: 0.910088; R: 0.910088; P: 0.910321; F1: 0.910196\n(valid @ 17): L: 0.234423; A: 0.868421; R: 0.868421; P: 0.871483; F1: 0.865992\n(train @ 18): L: 0.318216; A: 0.857456; R: 0.857456; P: 0.856543; F1: 0.856242\n(valid @ 18): L: 0.331429; A: 0.885965; R: 0.885965; P: 0.912460; F1: 0.885173\n(train @ 19): L: 0.248512; A: 0.896930; R: 0.896930; P: 0.899686; F1: 0.895773\n(valid @ 19): L: 0.276882; A: 0.881579; R: 0.881579; P: 0.883985; F1: 0.881623\n(train @ 20): L: 0.357652; A: 0.870614; R: 0.870614; P: 0.884050; F1: 0.871767\n(valid @ 20): L: 0.281950; A: 0.877193; R: 0.877193; P: 0.882451; F1: 0.875078\n(train @ 21): L: 0.246157; A: 0.901316; R: 0.901316; P: 0.908990; F1: 0.898456\n(valid @ 21): L: 0.337033; A: 0.859649; R: 0.859649; P: 0.864184; F1: 0.860899\n(train @ 22): L: 0.199422; A: 0.921053; R: 0.921053; P: 0.922355; F1: 0.921369\n(valid @ 22): L: 0.325866; A: 0.846491; R: 0.846491; P: 0.846847; F1: 0.846595\n(train @ 23): L: 0.188134; A: 0.934211; R: 0.934211; P: 0.934821; F1: 0.933917\n(valid @ 23): L: 0.217522; A: 0.894737; R: 0.894737; P: 0.897948; F1: 0.895293\n(train @ 24): L: 0.164524; A: 0.925439; R: 0.925439; P: 0.931102; F1: 0.925224\n(valid @ 24): L: 0.192815; A: 0.907895; R: 0.907895; P: 0.918726; F1: 0.907898\n(train @ 25): L: 0.205490; A: 0.905702; R: 0.905702; P: 0.907097; F1: 0.905454\n(valid @ 25): L: 0.194571; A: 0.903509; R: 0.903509; P: 0.907358; F1: 0.904032\n(train @ 26): L: 0.177724; A: 0.916667; R: 0.916667; P: 0.926218; F1: 0.915635\n(valid @ 26): L: 0.371094; A: 0.833333; R: 0.833333; P: 0.836439; F1: 0.833543\n(train @ 27): L: 0.174794; A: 0.940789; R: 0.940789; P: 0.940429; F1: 0.940528\n(valid @ 27): L: 0.219663; A: 0.916667; R: 0.916667; P: 0.929165; F1: 0.916710\n(train @ 28): L: 0.158117; A: 0.932018; R: 0.932018; P: 0.933716; F1: 0.931822\n(valid @ 28): L: 0.243568; A: 0.877193; R: 0.877193; P: 0.877587; F1: 0.876986\n(train @ 29): L: 0.144643; A: 0.934211; R: 0.934211; P: 0.933975; F1: 0.933939\n(valid @ 29): L: 0.179761; A: 0.916667; R: 0.916667; P: 0.926833; F1: 0.916359\n(train @ 30): L: 0.137558; A: 0.934211; R: 0.934211; P: 0.934706; F1: 0.934121\n(valid @ 30): L: 0.256669; A: 0.899123; R: 0.899123; P: 0.902945; F1: 0.900016\nBest val Metric 0.916710 @ 27\n\nmodel is saved @ ./predictions/200903163404_200910214753/ftype_1_2_4_5_7_8_pretrained_model.pt\nSaving predictions @ ./predictions/200903163404_200910214753/ftype_train_1_2_4_5_7_8_vggish.csv\nSaving predictions @ ./predictions/200903163404_200910214753/ftype_valid_3_6_9_vggish.csv\nSaving predictions @ ./predictions/200903163404_200910214753/ftype_test_trained_on_1_2_4_5_7_8_vggish.csv\n{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'test': [10, 11, 12]}\n(train @ 1): L: 1.463795; A: 0.311404; R: 0.311404; P: 0.314404; F1: 0.303712\n(valid @ 1): L: 1.412057; A: 0.364035; R: 0.364035; P: 0.309656; F1: 0.261692\n(train @ 2): L: 1.354274; A: 0.348684; R: 0.348684; P: 0.433198; F1: 0.272816\n(valid @ 2): L: 1.311451; A: 0.394737; R: 0.394737; P: 0.272161; F1: 0.313122\n(train @ 3): L: 1.343737; A: 0.320175; R: 0.320175; P: 0.245301; F1: 0.250675\n(valid @ 3): L: 1.311615; A: 0.403509; R: 0.403509; P: 0.297171; F1: 0.316206\n(train @ 4): L: 1.378471; A: 0.326754; R: 0.326754; P: 0.330725; F1: 0.297706\n(valid @ 4): L: 1.256554; A: 0.315789; R: 0.315789; P: 0.099723; F1: 0.151579\n(train @ 5): L: 1.271290; A: 0.353070; R: 0.353070; P: 0.378490; F1: 0.338063\n(valid @ 5): L: 1.195513; A: 0.372807; R: 0.372807; P: 0.364185; F1: 0.299166\n(train @ 6): L: 1.190326; A: 0.368421; R: 0.368421; P: 0.369884; F1: 0.363059\n(valid @ 6): L: 1.248829; A: 0.399123; R: 0.399123; P: 0.381564; F1: 0.345276\n(train @ 7): L: 1.233919; A: 0.359649; R: 0.359649; P: 0.357302; F1: 0.342710\n(valid @ 7): L: 1.152015; A: 0.407895; R: 0.407895; P: 0.405196; F1: 0.350973\n(train @ 8): L: 1.146071; A: 0.403509; R: 0.403509; P: 0.405825; F1: 0.394942\n(valid @ 8): L: 1.022720; A: 0.530702; R: 0.530702; P: 0.556975; F1: 0.505094\n(train @ 9): L: 1.085386; A: 0.541667; R: 0.541667; P: 0.565635; F1: 0.520026\n(valid @ 9): L: 0.889048; A: 0.679825; R: 0.679825; P: 0.729899; F1: 0.660792\n(train @ 10): L: 0.865110; A: 0.618421; R: 0.618421; P: 0.654734; F1: 0.592891\n(valid @ 10): L: 0.665477; A: 0.728070; R: 0.728070; P: 0.728365; F1: 0.727367\n(train @ 11): L: 0.602247; A: 0.732456; R: 0.732456; P: 0.735334; F1: 0.728180\n(valid @ 11): L: 0.511342; A: 0.785088; R: 0.785088; P: 0.810636; F1: 0.773718\n(train @ 12): L: 0.549570; A: 0.750000; R: 0.750000; P: 0.750697; F1: 0.744400\n(valid @ 12): L: 0.565023; A: 0.754386; R: 0.754386; P: 0.779347; F1: 0.747915\n(train @ 13): L: 0.486687; A: 0.804825; R: 0.804825; P: 0.804552; F1: 0.801648\n(valid @ 13): L: 0.337066; A: 0.859649; R: 0.859649; P: 0.869495; F1: 0.855506\n(train @ 14): L: 0.352523; A: 0.868421; R: 0.868421; P: 0.871014; F1: 0.866569\n(valid @ 14): L: 0.310246; A: 0.850877; R: 0.850877; P: 0.876090; F1: 0.844387\n(train @ 15): L: 0.329374; A: 0.877193; R: 0.877193; P: 0.880282; F1: 0.875886\n(valid @ 15): L: 0.349508; A: 0.846491; R: 0.846491; P: 0.875405; F1: 0.837407\n(train @ 16): L: 0.282384; A: 0.870614; R: 0.870614; P: 0.881414; F1: 0.869538\n(valid @ 16): L: 0.310440; A: 0.859649; R: 0.859649; P: 0.883485; F1: 0.852602\n(train @ 17): L: 0.242681; A: 0.896930; R: 0.896930; P: 0.896492; F1: 0.895686\n(valid @ 17): L: 0.331758; A: 0.868421; R: 0.868421; P: 0.887004; F1: 0.862999\n(train @ 18): L: 0.250162; A: 0.881579; R: 0.881579; P: 0.880975; F1: 0.880526\n(valid @ 18): L: 0.291591; A: 0.864035; R: 0.864035; P: 0.867569; F1: 0.859409\n(train @ 19): L: 0.240980; A: 0.892544; R: 0.892544; P: 0.893139; F1: 0.892529\n(valid @ 19): L: 0.213920; A: 0.899123; R: 0.899123; P: 0.908767; F1: 0.897575\n(train @ 20): L: 0.215412; A: 0.912281; R: 0.912281; P: 0.912660; F1: 0.912132\n(valid @ 20): L: 0.249179; A: 0.899123; R: 0.899123; P: 0.899348; F1: 0.897892\n(train @ 21): L: 0.197138; A: 0.910088; R: 0.910088; P: 0.914615; F1: 0.908945\n(valid @ 21): L: 0.209100; A: 0.885965; R: 0.885965; P: 0.889885; F1: 0.884429\n(train @ 22): L: 0.185541; A: 0.923246; R: 0.923246; P: 0.923313; F1: 0.923171\n(valid @ 22): L: 0.240815; A: 0.885965; R: 0.885965; P: 0.903683; F1: 0.883723\n(train @ 23): L: 0.135638; A: 0.949561; R: 0.949561; P: 0.949893; F1: 0.949445\n(valid @ 23): L: 0.178980; A: 0.899123; R: 0.899123; P: 0.909589; F1: 0.896825\n(train @ 24): L: 0.128024; A: 0.942982; R: 0.942982; P: 0.943069; F1: 0.942966\n(valid @ 24): L: 0.165661; A: 0.907895; R: 0.907895; P: 0.916167; F1: 0.907188\n(train @ 25): L: 0.106492; A: 0.953947; R: 0.953947; P: 0.955288; F1: 0.953892\n(valid @ 25): L: 0.167406; A: 0.899123; R: 0.899123; P: 0.903295; F1: 0.898870\n(train @ 26): L: 0.116411; A: 0.962719; R: 0.962719; P: 0.963611; F1: 0.962690\n(valid @ 26): L: 0.171339; A: 0.894737; R: 0.894737; P: 0.899667; F1: 0.894366\n(train @ 27): L: 0.096510; A: 0.967105; R: 0.967105; P: 0.967348; F1: 0.967097\n(valid @ 27): L: 0.191102; A: 0.916667; R: 0.916667; P: 0.931100; F1: 0.915452\n(train @ 28): L: 0.130013; A: 0.942982; R: 0.942982; P: 0.944365; F1: 0.943031\n(valid @ 28): L: 0.179075; A: 0.894737; R: 0.894737; P: 0.900125; F1: 0.893663\n(train @ 29): L: 0.108975; A: 0.953947; R: 0.953947; P: 0.953910; F1: 0.953881\n(valid @ 29): L: 0.195180; A: 0.894737; R: 0.894737; P: 0.903535; F1: 0.893946\n(train @ 30): L: 0.158901; A: 0.940789; R: 0.940789; P: 0.941655; F1: 0.941093\n(valid @ 30): L: 0.180264; A: 0.903509; R: 0.903509; P: 0.905090; F1: 0.902039\nBest val Metric 0.915452 @ 27\n\nmodel is saved @ ./predictions/200903163404_200910214753/ftype_1_3_4_6_7_9_pretrained_model.pt\nSaving predictions @ ./predictions/200903163404_200910214753/ftype_train_1_3_4_6_7_9_vggish.csv\nSaving predictions @ ./predictions/200903163404_200910214753/ftype_valid_2_5_8_vggish.csv\nSaving predictions @ ./predictions/200903163404_200910214753/ftype_test_trained_on_1_3_4_6_7_9_vggish.csv\n{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'test': [10, 11, 12]}\n(train @ 1): L: 1.447026; A: 0.315789; R: 0.315789; P: 0.320199; F1: 0.308581\n(valid @ 1): L: 1.437785; A: 0.333333; R: 0.333333; P: 0.249521; F1: 0.243581\n(train @ 2): L: 1.335825; A: 0.326754; R: 0.326754; P: 0.208503; F1: 0.249127\n(valid @ 2): L: 1.343691; A: 0.350877; R: 0.350877; P: 0.244304; F1: 0.273128\n(train @ 3): L: 1.289269; A: 0.339912; R: 0.339912; P: 0.342170; F1: 0.303089\n(valid @ 3): L: 1.355668; A: 0.372807; R: 0.372807; P: 0.330662; F1: 0.335993\n(train @ 4): L: 1.269640; A: 0.339912; R: 0.339912; P: 0.411708; F1: 0.321450\n(valid @ 4): L: 1.359984; A: 0.355263; R: 0.355263; P: 0.370706; F1: 0.338587\n(train @ 5): L: 1.197034; A: 0.401316; R: 0.401316; P: 0.411601; F1: 0.379057\n(valid @ 5): L: 1.427861; A: 0.320175; R: 0.320175; P: 0.335368; F1: 0.317287\n(train @ 6): L: 1.130921; A: 0.469298; R: 0.469298; P: 0.482241; F1: 0.458017\n(valid @ 6): L: 1.215108; A: 0.416667; R: 0.416667; P: 0.445735; F1: 0.406099\n(train @ 7): L: 1.077240; A: 0.414474; R: 0.414474; P: 0.473456; F1: 0.378190\n(valid @ 7): L: 1.066176; A: 0.500000; R: 0.500000; P: 0.509313; F1: 0.477293\n(train @ 8): L: 0.905460; A: 0.614035; R: 0.614035; P: 0.629726; F1: 0.603425\n(valid @ 8): L: 0.831699; A: 0.662281; R: 0.662281; P: 0.692915; F1: 0.630428\n(train @ 9): L: 0.656599; A: 0.697368; R: 0.697368; P: 0.697249; F1: 0.666256\n(valid @ 9): L: 1.039287; A: 0.592105; R: 0.592105; P: 0.694236; F1: 0.589801\n(train @ 10): L: 0.546781; A: 0.778509; R: 0.778509; P: 0.787110; F1: 0.766976\n(valid @ 10): L: 1.106240; A: 0.491228; R: 0.491228; P: 0.655607; F1: 0.419688\n(train @ 11): L: 0.584746; A: 0.730263; R: 0.730263; P: 0.737114; F1: 0.728071\n(valid @ 11): L: 0.579667; A: 0.741228; R: 0.741228; P: 0.774240; F1: 0.723979\n(train @ 12): L: 0.324743; A: 0.857456; R: 0.857456; P: 0.857405; F1: 0.855731\n(valid @ 12): L: 0.519828; A: 0.758772; R: 0.758772; P: 0.769612; F1: 0.751043\n(train @ 13): L: 0.263122; A: 0.892544; R: 0.892544; P: 0.891989; F1: 0.891925\n(valid @ 13): L: 0.356393; A: 0.850877; R: 0.850877; P: 0.857260; F1: 0.849158\n(train @ 14): L: 0.193905; A: 0.932018; R: 0.932018; P: 0.932014; F1: 0.932009\n(valid @ 14): L: 0.337826; A: 0.842105; R: 0.842105; P: 0.846196; F1: 0.839400\n(train @ 15): L: 0.237817; A: 0.918860; R: 0.918860; P: 0.919930; F1: 0.918081\n(valid @ 15): L: 0.406647; A: 0.833333; R: 0.833333; P: 0.837913; F1: 0.832112\n(train @ 16): L: 0.191497; A: 0.916667; R: 0.916667; P: 0.923697; F1: 0.915959\n(valid @ 16): L: 0.394313; A: 0.837719; R: 0.837719; P: 0.837515; F1: 0.834019\n(train @ 17): L: 0.184927; A: 0.923246; R: 0.923246; P: 0.924311; F1: 0.922829\n(valid @ 17): L: 0.417940; A: 0.815789; R: 0.815789; P: 0.827409; F1: 0.812110\n(train @ 18): L: 0.159524; A: 0.934211; R: 0.934211; P: 0.935919; F1: 0.934323\n(valid @ 18): L: 0.306650; A: 0.864035; R: 0.864035; P: 0.866043; F1: 0.861858\n(train @ 19): L: 0.133682; A: 0.947368; R: 0.947368; P: 0.947611; F1: 0.947109\n(valid @ 19): L: 0.392018; A: 0.859649; R: 0.859649; P: 0.863933; F1: 0.860381\n(train @ 20): L: 0.183692; A: 0.932018; R: 0.932018; P: 0.932660; F1: 0.932049\n(valid @ 20): L: 0.441220; A: 0.833333; R: 0.833333; P: 0.855428; F1: 0.825213\n(train @ 21): L: 0.164700; A: 0.942982; R: 0.942982; P: 0.943301; F1: 0.942913\n(valid @ 21): L: 0.284394; A: 0.859649; R: 0.859649; P: 0.865361; F1: 0.857854\n(train @ 22): L: 0.115255; A: 0.960526; R: 0.960526; P: 0.960674; F1: 0.960575\n(valid @ 22): L: 0.286154; A: 0.868421; R: 0.868421; P: 0.875739; F1: 0.865952\n(train @ 23): L: 0.079306; A: 0.971491; R: 0.971491; P: 0.971597; F1: 0.971535\n(valid @ 23): L: 0.294710; A: 0.868421; R: 0.868421; P: 0.873143; F1: 0.866096\n(train @ 24): L: 0.082918; A: 0.967105; R: 0.967105; P: 0.967528; F1: 0.967164\n(valid @ 24): L: 0.291871; A: 0.881579; R: 0.881579; P: 0.887223; F1: 0.880062\n(train @ 25): L: 0.067270; A: 0.975877; R: 0.975877; P: 0.975876; F1: 0.975869\n(valid @ 25): L: 0.301256; A: 0.885965; R: 0.885965; P: 0.900794; F1: 0.882955\n(train @ 26): L: 0.125241; A: 0.967105; R: 0.967105; P: 0.967927; F1: 0.967105\n(valid @ 26): L: 0.285843; A: 0.872807; R: 0.872807; P: 0.876757; F1: 0.871433\n(train @ 27): L: 0.052991; A: 0.980263; R: 0.980263; P: 0.980439; F1: 0.980298\n(valid @ 27): L: 0.285462; A: 0.894737; R: 0.894737; P: 0.911308; F1: 0.892227\n(train @ 28): L: 0.055603; A: 0.971491; R: 0.971491; P: 0.971965; F1: 0.971502\n(valid @ 28): L: 0.269410; A: 0.903509; R: 0.903509; P: 0.907772; F1: 0.902356\n(train @ 29): L: 0.059963; A: 0.973684; R: 0.973684; P: 0.973775; F1: 0.973720\n(valid @ 29): L: 0.244102; A: 0.899123; R: 0.899123; P: 0.905143; F1: 0.897812\n(train @ 30): L: 0.046977; A: 0.982456; R: 0.982456; P: 0.982456; F1: 0.982456\n(valid @ 30): L: 0.242103; A: 0.907895; R: 0.907895; P: 0.914300; F1: 0.906707\nBest val Metric 0.906707 @ 30\n\nmodel is saved @ ./predictions/200903163404_200910214753/ftype_2_3_5_6_8_9_pretrained_model.pt\nSaving predictions @ ./predictions/200903163404_200910214753/ftype_train_2_3_5_6_8_9_vggish.csv\nSaving predictions @ ./predictions/200903163404_200910214753/ftype_valid_1_4_7_vggish.csv\nSaving predictions @ ./predictions/200903163404_200910214753/ftype_test_trained_on_2_3_5_6_8_9_vggish.csv\nAverage of Best Metrics on Each Valid Set: 0.912957, 200903163404_200910214753\n"
    }
   ],
   "source": [
    "# Reproduce the best experiment\n",
    "import sys\n",
    "sys.path.insert(0, '../../filling_level/vggish/')  # nopep8\n",
    "import argparse\n",
    "from main import Config, run_kfold\n",
    "from time import localtime, strftime\n",
    "\n",
    "# if True, will use the pre-trained model and make predictions, if False, will train the model\n",
    "use_pretrained = False\n",
    "exp_name = 200903163404\n",
    "cfg = Config()\n",
    "cfg.load_from(path=f'./predictions/{exp_name}/cfg.txt')\n",
    "# replacing the time with the old_time + current_time such that there is no collision\n",
    "if use_pretrained:\n",
    "    cfg.init_time = exp_name\n",
    "else:\n",
    "    cfg.init_time = f'{cfg.init_time}_{strftime(\"%y%m%d%H%M%S\", localtime())}'\n",
    "run_kfold(cfg, use_pretrained)  # Expected average of Best Metrics on Each Valid Set: 0.912957 @ 200903163404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'test': [10, 11, 12]}\n(train @ 1): L: 1.435528; A: 0.296053; R: 0.296053; P: 0.297282; F1: 0.284392\n(valid @ 1): L: 1.403968; A: 0.346491; R: 0.346491; P: 0.274202; F1: 0.246877\n(train @ 2): L: 1.355512; A: 0.326754; R: 0.326754; P: 0.265790; F1: 0.288936\n(valid @ 2): L: 1.347115; A: 0.385965; R: 0.385965; P: 0.182825; F1: 0.248120\n(train @ 3): L: 1.338570; A: 0.350877; R: 0.350877; P: 0.280349; F1: 0.308697\n(valid @ 3): L: 1.297946; A: 0.412281; R: 0.412281; P: 0.307723; F1: 0.324470\n(train @ 4): L: 1.328715; A: 0.326754; R: 0.326754; P: 0.335725; F1: 0.304643\n(valid @ 4): L: 1.268678; A: 0.315789; R: 0.315789; P: 0.099723; F1: 0.151579\n(train @ 5): L: 1.246402; A: 0.377193; R: 0.377193; P: 0.397946; F1: 0.356769\n(valid @ 5): L: 1.115692; A: 0.504386; R: 0.504386; P: 0.527962; F1: 0.480276\n(train @ 6): L: 1.187328; A: 0.388158; R: 0.388158; P: 0.398701; F1: 0.376050\n(valid @ 6): L: 1.118171; A: 0.425439; R: 0.425439; P: 0.364662; F1: 0.368563\n(train @ 7): L: 1.189692; A: 0.383772; R: 0.383772; P: 0.374229; F1: 0.348271\n(valid @ 7): L: 1.123886; A: 0.521930; R: 0.521930; P: 0.596201; F1: 0.487944\n(train @ 8): L: 1.044008; A: 0.489035; R: 0.489035; P: 0.478644; F1: 0.479735\n(valid @ 8): L: 0.885678; A: 0.723684; R: 0.723684; P: 0.745728; F1: 0.724340\n(train @ 9): L: 0.858133; A: 0.642544; R: 0.642544; P: 0.692780; F1: 0.618590\n(valid @ 9): L: 0.732799; A: 0.692982; R: 0.692982; P: 0.715715; F1: 0.670914\n(train @ 10): L: 0.699228; A: 0.714912; R: 0.714912; P: 0.737887; F1: 0.687563\n(valid @ 10): L: 0.663705; A: 0.750000; R: 0.750000; P: 0.774644; F1: 0.747244\n(train @ 11): L: 0.768314; A: 0.699561; R: 0.699561; P: 0.694733; F1: 0.688113\n(valid @ 11): L: 0.428039; A: 0.815789; R: 0.815789; P: 0.813404; F1: 0.813249\n(train @ 12): L: 0.426726; A: 0.809211; R: 0.809211; P: 0.808940; F1: 0.803345\n(valid @ 12): L: 0.522549; A: 0.754386; R: 0.754386; P: 0.768031; F1: 0.753524\n(train @ 13): L: 0.518313; A: 0.802632; R: 0.802632; P: 0.801180; F1: 0.800668\n(valid @ 13): L: 0.299965; A: 0.842105; R: 0.842105; P: 0.842538; F1: 0.838935\n(train @ 14): L: 0.383844; A: 0.833333; R: 0.833333; P: 0.836189; F1: 0.830651\n(valid @ 14): L: 0.359081; A: 0.868421; R: 0.868421; P: 0.897771; F1: 0.869235\n(train @ 15): L: 0.298331; A: 0.866228; R: 0.866228; P: 0.869331; F1: 0.864437\n(valid @ 15): L: 0.240245; A: 0.881579; R: 0.881579; P: 0.893559; F1: 0.876506\n(train @ 16): L: 0.242944; A: 0.896930; R: 0.896930; P: 0.903873; F1: 0.894593\n(valid @ 16): L: 0.253270; A: 0.885965; R: 0.885965; P: 0.890694; F1: 0.886409\n(train @ 17): L: 0.225206; A: 0.910088; R: 0.910088; P: 0.910321; F1: 0.910196\n(valid @ 17): L: 0.234423; A: 0.868421; R: 0.868421; P: 0.871483; F1: 0.865992\n(train @ 18): L: 0.318216; A: 0.857456; R: 0.857456; P: 0.856543; F1: 0.856242\n(valid @ 18): L: 0.331429; A: 0.885965; R: 0.885965; P: 0.912460; F1: 0.885173\n(train @ 19): L: 0.248512; A: 0.896930; R: 0.896930; P: 0.899686; F1: 0.895773\n(valid @ 19): L: 0.276882; A: 0.881579; R: 0.881579; P: 0.883985; F1: 0.881623\n(train @ 20): L: 0.357652; A: 0.870614; R: 0.870614; P: 0.884050; F1: 0.871767\n(valid @ 20): L: 0.281950; A: 0.877193; R: 0.877193; P: 0.882451; F1: 0.875078\n(train @ 21): L: 0.246157; A: 0.901316; R: 0.901316; P: 0.908990; F1: 0.898456\n(valid @ 21): L: 0.337033; A: 0.859649; R: 0.859649; P: 0.864184; F1: 0.860899\n(train @ 22): L: 0.199422; A: 0.921053; R: 0.921053; P: 0.922355; F1: 0.921369\n(valid @ 22): L: 0.325866; A: 0.846491; R: 0.846491; P: 0.846847; F1: 0.846595\n(train @ 23): L: 0.188134; A: 0.934211; R: 0.934211; P: 0.934821; F1: 0.933917\n(valid @ 23): L: 0.217522; A: 0.894737; R: 0.894737; P: 0.897948; F1: 0.895293\n(train @ 24): L: 0.164524; A: 0.925439; R: 0.925439; P: 0.931102; F1: 0.925224\n(valid @ 24): L: 0.192815; A: 0.907895; R: 0.907895; P: 0.918726; F1: 0.907898\n(train @ 25): L: 0.205490; A: 0.905702; R: 0.905702; P: 0.907097; F1: 0.905454\n(valid @ 25): L: 0.194571; A: 0.903509; R: 0.903509; P: 0.907358; F1: 0.904032\n(train @ 26): L: 0.177724; A: 0.916667; R: 0.916667; P: 0.926218; F1: 0.915635\n(valid @ 26): L: 0.371094; A: 0.833333; R: 0.833333; P: 0.836439; F1: 0.833543\n(train @ 27): L: 0.174794; A: 0.940789; R: 0.940789; P: 0.940429; F1: 0.940528\n(valid @ 27): L: 0.219663; A: 0.916667; R: 0.916667; P: 0.929165; F1: 0.916710\n(train @ 28): L: 0.158117; A: 0.932018; R: 0.932018; P: 0.933716; F1: 0.931822\n(valid @ 28): L: 0.243568; A: 0.877193; R: 0.877193; P: 0.877587; F1: 0.876986\n(train @ 29): L: 0.144643; A: 0.934211; R: 0.934211; P: 0.933975; F1: 0.933939\n(valid @ 29): L: 0.179761; A: 0.916667; R: 0.916667; P: 0.926833; F1: 0.916359\n(train @ 30): L: 0.137558; A: 0.934211; R: 0.934211; P: 0.934706; F1: 0.934121\n(valid @ 30): L: 0.256669; A: 0.899123; R: 0.899123; P: 0.902945; F1: 0.900016\nBest val Metric 0.916710 @ 27\n\n{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'test': [10, 11, 12]}\n(train @ 1): L: 1.463795; A: 0.311404; R: 0.311404; P: 0.314404; F1: 0.303712\n(valid @ 1): L: 1.412057; A: 0.364035; R: 0.364035; P: 0.309656; F1: 0.261692\n(train @ 2): L: 1.354274; A: 0.348684; R: 0.348684; P: 0.433198; F1: 0.272816\n(valid @ 2): L: 1.311451; A: 0.394737; R: 0.394737; P: 0.272161; F1: 0.313122\n(train @ 3): L: 1.343737; A: 0.320175; R: 0.320175; P: 0.245301; F1: 0.250675\n(valid @ 3): L: 1.311615; A: 0.403509; R: 0.403509; P: 0.297171; F1: 0.316206\n(train @ 4): L: 1.378471; A: 0.326754; R: 0.326754; P: 0.330725; F1: 0.297706\n(valid @ 4): L: 1.256554; A: 0.315789; R: 0.315789; P: 0.099723; F1: 0.151579\n(train @ 5): L: 1.271290; A: 0.353070; R: 0.353070; P: 0.378490; F1: 0.338063\n(valid @ 5): L: 1.195513; A: 0.372807; R: 0.372807; P: 0.364185; F1: 0.299166\n(train @ 6): L: 1.190326; A: 0.368421; R: 0.368421; P: 0.369884; F1: 0.363059\n(valid @ 6): L: 1.248829; A: 0.399123; R: 0.399123; P: 0.381564; F1: 0.345276\n(train @ 7): L: 1.233919; A: 0.359649; R: 0.359649; P: 0.357302; F1: 0.342710\n(valid @ 7): L: 1.152015; A: 0.407895; R: 0.407895; P: 0.405196; F1: 0.350973\n(train @ 8): L: 1.146071; A: 0.403509; R: 0.403509; P: 0.405825; F1: 0.394942\n(valid @ 8): L: 1.022720; A: 0.530702; R: 0.530702; P: 0.556975; F1: 0.505094\n(train @ 9): L: 1.085386; A: 0.541667; R: 0.541667; P: 0.565635; F1: 0.520026\n(valid @ 9): L: 0.889048; A: 0.679825; R: 0.679825; P: 0.729899; F1: 0.660792\n(train @ 10): L: 0.865110; A: 0.618421; R: 0.618421; P: 0.654734; F1: 0.592891\n(valid @ 10): L: 0.665477; A: 0.728070; R: 0.728070; P: 0.728365; F1: 0.727367\n(train @ 11): L: 0.602247; A: 0.732456; R: 0.732456; P: 0.735334; F1: 0.728180\n(valid @ 11): L: 0.511342; A: 0.785088; R: 0.785088; P: 0.810636; F1: 0.773718\n(train @ 12): L: 0.549570; A: 0.750000; R: 0.750000; P: 0.750697; F1: 0.744400\n(valid @ 12): L: 0.565023; A: 0.754386; R: 0.754386; P: 0.779347; F1: 0.747915\n(train @ 13): L: 0.486687; A: 0.804825; R: 0.804825; P: 0.804552; F1: 0.801648\n(valid @ 13): L: 0.337066; A: 0.859649; R: 0.859649; P: 0.869495; F1: 0.855506\n(train @ 14): L: 0.352523; A: 0.868421; R: 0.868421; P: 0.871014; F1: 0.866569\n(valid @ 14): L: 0.310246; A: 0.850877; R: 0.850877; P: 0.876090; F1: 0.844387\n(train @ 15): L: 0.329374; A: 0.877193; R: 0.877193; P: 0.880282; F1: 0.875886\n(valid @ 15): L: 0.349508; A: 0.846491; R: 0.846491; P: 0.875405; F1: 0.837407\n(train @ 16): L: 0.282384; A: 0.870614; R: 0.870614; P: 0.881414; F1: 0.869538\n(valid @ 16): L: 0.310440; A: 0.859649; R: 0.859649; P: 0.883485; F1: 0.852602\n(train @ 17): L: 0.242681; A: 0.896930; R: 0.896930; P: 0.896492; F1: 0.895686\n(valid @ 17): L: 0.331758; A: 0.868421; R: 0.868421; P: 0.887004; F1: 0.862999\n(train @ 18): L: 0.250162; A: 0.881579; R: 0.881579; P: 0.880975; F1: 0.880526\n(valid @ 18): L: 0.291591; A: 0.864035; R: 0.864035; P: 0.867569; F1: 0.859409\n(train @ 19): L: 0.240980; A: 0.892544; R: 0.892544; P: 0.893139; F1: 0.892529\n(valid @ 19): L: 0.213920; A: 0.899123; R: 0.899123; P: 0.908767; F1: 0.897575\n(train @ 20): L: 0.215412; A: 0.912281; R: 0.912281; P: 0.912660; F1: 0.912132\n(valid @ 20): L: 0.249179; A: 0.899123; R: 0.899123; P: 0.899348; F1: 0.897892\n(train @ 21): L: 0.197138; A: 0.910088; R: 0.910088; P: 0.914615; F1: 0.908945\n(valid @ 21): L: 0.209100; A: 0.885965; R: 0.885965; P: 0.889885; F1: 0.884429\n(train @ 22): L: 0.185541; A: 0.923246; R: 0.923246; P: 0.923313; F1: 0.923171\n(valid @ 22): L: 0.240815; A: 0.885965; R: 0.885965; P: 0.903683; F1: 0.883723\n(train @ 23): L: 0.135638; A: 0.949561; R: 0.949561; P: 0.949893; F1: 0.949445\n(valid @ 23): L: 0.178980; A: 0.899123; R: 0.899123; P: 0.909589; F1: 0.896825\n(train @ 24): L: 0.128024; A: 0.942982; R: 0.942982; P: 0.943069; F1: 0.942966\n(valid @ 24): L: 0.165661; A: 0.907895; R: 0.907895; P: 0.916167; F1: 0.907188\n(train @ 25): L: 0.106492; A: 0.953947; R: 0.953947; P: 0.955288; F1: 0.953892\n(valid @ 25): L: 0.167406; A: 0.899123; R: 0.899123; P: 0.903295; F1: 0.898870\n(train @ 26): L: 0.116411; A: 0.962719; R: 0.962719; P: 0.963611; F1: 0.962690\n(valid @ 26): L: 0.171339; A: 0.894737; R: 0.894737; P: 0.899667; F1: 0.894366\n(train @ 27): L: 0.096510; A: 0.967105; R: 0.967105; P: 0.967348; F1: 0.967097\n(valid @ 27): L: 0.191102; A: 0.916667; R: 0.916667; P: 0.931100; F1: 0.915452\n(train @ 28): L: 0.130013; A: 0.942982; R: 0.942982; P: 0.944365; F1: 0.943031\n(valid @ 28): L: 0.179075; A: 0.894737; R: 0.894737; P: 0.900125; F1: 0.893663\n(train @ 29): L: 0.108975; A: 0.953947; R: 0.953947; P: 0.953910; F1: 0.953881\n(valid @ 29): L: 0.195180; A: 0.894737; R: 0.894737; P: 0.903535; F1: 0.893946\n(train @ 30): L: 0.158901; A: 0.940789; R: 0.940789; P: 0.941655; F1: 0.941093\n(valid @ 30): L: 0.180264; A: 0.903509; R: 0.903509; P: 0.905090; F1: 0.902039\nBest val Metric 0.915452 @ 27\n\n{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'test': [10, 11, 12]}\n(train @ 1): L: 1.447026; A: 0.315789; R: 0.315789; P: 0.320199; F1: 0.308581\n(valid @ 1): L: 1.437785; A: 0.333333; R: 0.333333; P: 0.249521; F1: 0.243581\n(train @ 2): L: 1.335825; A: 0.326754; R: 0.326754; P: 0.208503; F1: 0.249127\n(valid @ 2): L: 1.343691; A: 0.350877; R: 0.350877; P: 0.244304; F1: 0.273128\n(train @ 3): L: 1.289269; A: 0.339912; R: 0.339912; P: 0.342170; F1: 0.303089\n(valid @ 3): L: 1.355668; A: 0.372807; R: 0.372807; P: 0.330662; F1: 0.335993\n(train @ 4): L: 1.269640; A: 0.339912; R: 0.339912; P: 0.411708; F1: 0.321450\n(valid @ 4): L: 1.359984; A: 0.355263; R: 0.355263; P: 0.370706; F1: 0.338587\n(train @ 5): L: 1.197034; A: 0.401316; R: 0.401316; P: 0.411601; F1: 0.379057\n(valid @ 5): L: 1.427861; A: 0.320175; R: 0.320175; P: 0.335368; F1: 0.317287\n(train @ 6): L: 1.130921; A: 0.469298; R: 0.469298; P: 0.482241; F1: 0.458017\n(valid @ 6): L: 1.215108; A: 0.416667; R: 0.416667; P: 0.445735; F1: 0.406099\n(train @ 7): L: 1.077240; A: 0.414474; R: 0.414474; P: 0.473456; F1: 0.378190\n(valid @ 7): L: 1.066176; A: 0.500000; R: 0.500000; P: 0.509313; F1: 0.477293\n(train @ 8): L: 0.905460; A: 0.614035; R: 0.614035; P: 0.629726; F1: 0.603425\n(valid @ 8): L: 0.831699; A: 0.662281; R: 0.662281; P: 0.692915; F1: 0.630428\n(train @ 9): L: 0.656599; A: 0.697368; R: 0.697368; P: 0.697249; F1: 0.666256\n(valid @ 9): L: 1.039287; A: 0.592105; R: 0.592105; P: 0.694236; F1: 0.589801\n(train @ 10): L: 0.546781; A: 0.778509; R: 0.778509; P: 0.787110; F1: 0.766976\n(valid @ 10): L: 1.106240; A: 0.491228; R: 0.491228; P: 0.655607; F1: 0.419688\n(train @ 11): L: 0.584746; A: 0.730263; R: 0.730263; P: 0.737114; F1: 0.728071\n(valid @ 11): L: 0.579667; A: 0.741228; R: 0.741228; P: 0.774240; F1: 0.723979\n(train @ 12): L: 0.324743; A: 0.857456; R: 0.857456; P: 0.857405; F1: 0.855731\n(valid @ 12): L: 0.519828; A: 0.758772; R: 0.758772; P: 0.769612; F1: 0.751043\n(train @ 13): L: 0.263122; A: 0.892544; R: 0.892544; P: 0.891989; F1: 0.891925\n(valid @ 13): L: 0.356393; A: 0.850877; R: 0.850877; P: 0.857260; F1: 0.849158\n(train @ 14): L: 0.193905; A: 0.932018; R: 0.932018; P: 0.932014; F1: 0.932009\n(valid @ 14): L: 0.337826; A: 0.842105; R: 0.842105; P: 0.846196; F1: 0.839400\n(train @ 15): L: 0.237817; A: 0.918860; R: 0.918860; P: 0.919930; F1: 0.918081\n(valid @ 15): L: 0.406647; A: 0.833333; R: 0.833333; P: 0.837913; F1: 0.832112\n(train @ 16): L: 0.191497; A: 0.916667; R: 0.916667; P: 0.923697; F1: 0.915959\n(valid @ 16): L: 0.394313; A: 0.837719; R: 0.837719; P: 0.837515; F1: 0.834019\n(train @ 17): L: 0.184927; A: 0.923246; R: 0.923246; P: 0.924311; F1: 0.922829\n(valid @ 17): L: 0.417940; A: 0.815789; R: 0.815789; P: 0.827409; F1: 0.812110\n(train @ 18): L: 0.159524; A: 0.934211; R: 0.934211; P: 0.935919; F1: 0.934323\n(valid @ 18): L: 0.306650; A: 0.864035; R: 0.864035; P: 0.866043; F1: 0.861858\n(train @ 19): L: 0.133682; A: 0.947368; R: 0.947368; P: 0.947611; F1: 0.947109\n(valid @ 19): L: 0.392018; A: 0.859649; R: 0.859649; P: 0.863933; F1: 0.860381\n(train @ 20): L: 0.183692; A: 0.932018; R: 0.932018; P: 0.932660; F1: 0.932049\n(valid @ 20): L: 0.441220; A: 0.833333; R: 0.833333; P: 0.855428; F1: 0.825213\n(train @ 21): L: 0.164700; A: 0.942982; R: 0.942982; P: 0.943301; F1: 0.942913\n(valid @ 21): L: 0.284394; A: 0.859649; R: 0.859649; P: 0.865361; F1: 0.857854\n(train @ 22): L: 0.115255; A: 0.960526; R: 0.960526; P: 0.960674; F1: 0.960575\n(valid @ 22): L: 0.286154; A: 0.868421; R: 0.868421; P: 0.875739; F1: 0.865952\n(train @ 23): L: 0.079306; A: 0.971491; R: 0.971491; P: 0.971597; F1: 0.971535\n(valid @ 23): L: 0.294710; A: 0.868421; R: 0.868421; P: 0.873143; F1: 0.866096\n(train @ 24): L: 0.082918; A: 0.967105; R: 0.967105; P: 0.967528; F1: 0.967164\n(valid @ 24): L: 0.291871; A: 0.881579; R: 0.881579; P: 0.887223; F1: 0.880062\n(train @ 25): L: 0.067270; A: 0.975877; R: 0.975877; P: 0.975876; F1: 0.975869\n(valid @ 25): L: 0.301256; A: 0.885965; R: 0.885965; P: 0.900794; F1: 0.882955\n(train @ 26): L: 0.125241; A: 0.967105; R: 0.967105; P: 0.967927; F1: 0.967105\n(valid @ 26): L: 0.285843; A: 0.872807; R: 0.872807; P: 0.876757; F1: 0.871433\n(train @ 27): L: 0.052991; A: 0.980263; R: 0.980263; P: 0.980439; F1: 0.980298\n(valid @ 27): L: 0.285462; A: 0.894737; R: 0.894737; P: 0.911308; F1: 0.892227\n(train @ 28): L: 0.055603; A: 0.971491; R: 0.971491; P: 0.971965; F1: 0.971502\n(valid @ 28): L: 0.269410; A: 0.903509; R: 0.903509; P: 0.907772; F1: 0.902356\n(train @ 29): L: 0.059963; A: 0.973684; R: 0.973684; P: 0.973775; F1: 0.973720\n(valid @ 29): L: 0.244102; A: 0.899123; R: 0.899123; P: 0.905143; F1: 0.897812\n(train @ 30): L: 0.046977; A: 0.982456; R: 0.982456; P: 0.982456; F1: 0.982456\n(valid @ 30): L: 0.242103; A: 0.907895; R: 0.907895; P: 0.914300; F1: 0.906707\nBest val Metric 0.906707 @ 30\n\nAverage of Best Metrics on Each Valid Set: 0.912957, 200903163404\n"
    }
   ],
   "source": [
    "# Experiment with other parameters\n",
    "import sys\n",
    "sys.path.insert(0, '../../filling_level/vggish/')  # nopep8\n",
    "\n",
    "import argparse\n",
    "from main import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'ftype')\n",
    "cfg.assign_variable('output_dim', 4)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', '../../filling_level/vggish/vggish_features')\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 128)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 5)\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}